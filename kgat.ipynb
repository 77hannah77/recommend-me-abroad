{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba2e4c59",
   "metadata": {},
   "source": [
    "# 2. KGAT: Knowledge Graph Attention Network for Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7a8c1",
   "metadata": {},
   "source": [
    "## 2.1. Aggregator and KGAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0616170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# L2 regularization loss, calculates the mean of squared values for a tensor\n",
    "def _L2_loss_mean(x):\n",
    "    return torch.mean(torch.sum(torch.pow(x, 2), dim=1, keepdim=False) / 2.)\n",
    "\n",
    "# Aggregator class for message-passing in GNN\n",
    "class Aggregator(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim, dropout, aggregator_type):\n",
    "        super(Aggregator, self).__init__()\n",
    "        # Initialize dimensions, dropout, and type of aggregator\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.dropout = dropout\n",
    "        self.aggregator_type = aggregator_type\n",
    "\n",
    "        # Dropout for message-passing and activation function\n",
    "        self.message_dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "\n",
    "        # Initialize linear transformations based on aggregator type\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            # GCN type: single linear layer\n",
    "            self.linear = nn.Linear(self.in_dim, self.out_dim)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            # GraphSAGE type: concatenates input and neighbor embeddings\n",
    "            self.linear = nn.Linear(self.in_dim * 2, self.out_dim)\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            # Bi-interaction type: uses two linear transformations\n",
    "            self.linear1 = nn.Linear(self.in_dim, self.out_dim)\n",
    "            self.linear2 = nn.Linear(self.in_dim, self.out_dim)\n",
    "            nn.init.xavier_uniform_(self.linear1.weight)\n",
    "            nn.init.xavier_uniform_(self.linear2.weight)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    # Forward pass for aggregator\n",
    "    def forward(self, ego_embeddings, A_in):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        ego_embeddings: user and entity embeddings\n",
    "        A_in: adjacency matrix as a sparse tensor\n",
    "        \"\"\"\n",
    "        # Neighbor aggregation\n",
    "        side_embeddings = torch.matmul(A_in, ego_embeddings)\n",
    "\n",
    "        if self.aggregator_type == 'gcn':\n",
    "            embeddings = ego_embeddings + side_embeddings\n",
    "            embeddings = self.activation(self.linear(embeddings))\n",
    "        elif self.aggregator_type == 'graphsage':\n",
    "            embeddings = torch.cat([ego_embeddings, side_embeddings], dim=1)\n",
    "            embeddings = self.activation(self.linear(embeddings))\n",
    "        elif self.aggregator_type == 'bi-interaction':\n",
    "            sum_embeddings = self.activation(self.linear1(ego_embeddings + side_embeddings))\n",
    "            bi_embeddings = self.activation(self.linear2(ego_embeddings * side_embeddings))\n",
    "            embeddings = bi_embeddings + sum_embeddings\n",
    "\n",
    "        embeddings = self.message_dropout(embeddings) \n",
    "        return embeddings\n",
    "\n",
    "# Knowledge Graph Attention Network (KGAT) model\n",
    "class KGAT(nn.Module):\n",
    "    \n",
    "    def __init__(self, args, n_users, n_entities, n_relations, A_in=None, user_pre_embed=None, item_pre_embed=None):\n",
    "        super(KGAT, self).__init__()\n",
    "        \n",
    "        # Initialize user and entity embeddings, relation embeddings\n",
    "        self.embed_dim = args.embed_dim\n",
    "        self.user_entity_embed = nn.Embedding(n_users + n_entities, self.embed_dim)\n",
    "        self.relation_embed = nn.Embedding(n_relations, self.embed_dim)\n",
    "        self.use_pretrain = args.use_pretrain\n",
    "\n",
    "        # Initialize hyperparameters and structure settings\n",
    "        self.n_users = n_users\n",
    "        self.n_entities = n_entities\n",
    "        self.n_relations = n_relations\n",
    "        self.relation_dim = args.relation_dim\n",
    "        self.aggregation_type = args.aggregation_type\n",
    "        self.conv_dim_list = [args.embed_dim] + eval(args.conv_dim_list)\n",
    "        self.mess_dropout = eval(args.mess_dropout)\n",
    "        self.n_layers = len(eval(args.conv_dim_list))\n",
    "        self.kg_l2loss_lambda = args.kg_l2loss_lambda\n",
    "        self.cf_l2loss_lambda = args.cf_l2loss_lambda\n",
    "\n",
    "        # Initialize transformation matrices and embeddings\n",
    "        self.entity_user_embed = nn.Embedding(self.n_entities + self.n_users, self.embed_dim)\n",
    "        self.relation_embed = nn.Embedding(self.n_relations, self.relation_dim)\n",
    "        self.trans_M = nn.Parameter(torch.Tensor(self.n_relations, self.embed_dim, self.relation_dim))\n",
    "\n",
    "        # Pretrain embedding weights if available\n",
    "        if (self.use_pretrain == 1) and (user_pre_embed is not None) and (item_pre_embed is not None):\n",
    "            other_entity_embed = nn.Parameter(torch.Tensor(self.n_entities - item_pre_embed.shape[0], self.embed_dim))\n",
    "            nn.init.xavier_uniform_(other_entity_embed)\n",
    "            entity_user_embed = torch.cat([item_pre_embed, other_entity_embed, user_pre_embed], dim=0)\n",
    "            self.entity_user_embed.weight = nn.Parameter(entity_user_embed)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.entity_user_embed.weight)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_uniform_(self.relation_embed.weight)\n",
    "        nn.init.xavier_uniform_(self.trans_M)\n",
    "\n",
    "        # Aggregator layers for multi-layer GNN\n",
    "        self.aggregator_layers = nn.ModuleList()\n",
    "        for k in range(self.n_layers):\n",
    "            self.aggregator_layers.append(Aggregator(self.conv_dim_list[k], self.conv_dim_list[k + 1], self.mess_dropout[k], self.aggregation_type))\n",
    "\n",
    "        # Initialize sparse adjacency matrix\n",
    "        self.A_in = nn.Parameter(torch.sparse.FloatTensor(self.n_users + self.n_entities, self.n_users + self.n_entities))\n",
    "        if A_in is not None:\n",
    "            self.A_in.data = A_in\n",
    "        self.A_in.requires_grad = False\n",
    "\n",
    "    # Calculate collaborative filtering embeddings\n",
    "    def calc_cf_embeddings(self):\n",
    "        ego_embed = self.entity_user_embed.weight\n",
    "        all_embed = [ego_embed]\n",
    "\n",
    "        for idx, layer in enumerate(self.aggregator_layers):\n",
    "            ego_embed = layer(ego_embed, self.A_in)\n",
    "            norm_embed = F.normalize(ego_embed, p=2, dim=1)\n",
    "            all_embed.append(norm_embed)\n",
    "\n",
    "        all_embed = torch.cat(all_embed, dim=1)\n",
    "        return all_embed\n",
    "\n",
    "    # Calculate collaborative filtering loss\n",
    "    def calc_cf_loss(self, user_ids, item_pos_ids, item_neg_ids):\n",
    "        all_embed = self.calc_cf_embeddings()\n",
    "        user_embed = all_embed[user_ids]\n",
    "        item_pos_embed = all_embed[item_pos_ids]\n",
    "        item_neg_embed = all_embed[item_neg_ids]\n",
    "\n",
    "        pos_score = torch.sum(user_embed * item_pos_embed, dim=1)\n",
    "        neg_score = torch.sum(user_embed * item_neg_embed, dim=1)\n",
    "\n",
    "        cf_loss = (-1.0) * F.logsigmoid(pos_score - neg_score)\n",
    "        cf_loss = torch.mean(cf_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(user_embed) + _L2_loss_mean(item_pos_embed) + _L2_loss_mean(item_neg_embed)\n",
    "        loss = cf_loss + self.cf_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "    # Calculate knowledge graph loss\n",
    "    def calc_kg_loss(self, h, r, pos_t, neg_t):\n",
    "        r_embed = self.relation_embed(r)\n",
    "        W_r = self.trans_M[r]\n",
    "\n",
    "        h_embed = self.entity_user_embed(h)\n",
    "        pos_t_embed = self.entity_user_embed(pos_t)\n",
    "        neg_t_embed = self.entity_user_embed(neg_t)\n",
    "\n",
    "        r_mul_h = torch.bmm(h_embed.unsqueeze(1), W_r).squeeze(1)\n",
    "        r_mul_pos_t = torch.bmm(pos_t_embed.unsqueeze(1), W_r).squeeze(1)\n",
    "        r_mul_neg_t = torch.bmm(neg_t_embed.unsqueeze(1), W_r).squeeze(1)\n",
    "\n",
    "        pos_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_pos_t, 2), dim=1)\n",
    "        neg_score = torch.sum(torch.pow(r_mul_h + r_embed - r_mul_neg_t, 2), dim=1)\n",
    "\n",
    "        kg_loss = (-1.0) * F.logsigmoid(neg_score - pos_score)\n",
    "        kg_loss = torch.mean(kg_loss)\n",
    "\n",
    "        l2_loss = _L2_loss_mean(r_mul_h) + _L2_loss_mean(r_embed) + _L2_loss_mean(r_mul_pos_t) + _L2_loss_mean(r_mul_neg_t)\n",
    "        loss = kg_loss + self.kg_l2loss_lambda * l2_loss\n",
    "        return loss\n",
    "\n",
    "    # Update attention scores for entities and relations\n",
    "    def update_attention_batch(self, h_list, t_list, r_idx):\n",
    "        r_embed = self.relation_embed.weight[r_idx]\n",
    "        W_r = self.trans_M[r_idx]\n",
    "\n",
    "        h_embed = self.entity_user_embed.weight[h_list]\n",
    "        t_embed = self.entity_user_embed.weight[t_list]\n",
    "\n",
    "        r_mul_h = torch.matmul(h_embed, W_r)\n",
    "        r_mul_t = torch.matmul(t_embed, W_r)\n",
    "        v_list = torch.sum(r_mul_t * torch.tanh(r_mul_h + r_embed), dim=1)\n",
    "        return v_list\n",
    "\n",
    "    # Calculate attention for the entire graph\n",
    "    def update_attention(self, h_list, t_list, r_list, relations):\n",
    "        device = self.A_in.device\n",
    "\n",
    "        rows, cols, values = [], [], []\n",
    "\n",
    "        for r_idx in relations:\n",
    "            index_list = torch.where(r_list == r_idx)\n",
    "            batch_h_list = h_list[index_list]\n",
    "            batch_t_list = t_list[index_list]\n",
    "\n",
    "            batch_v_list = self.update_attention_batch(batch_h_list, batch_t_list, r_idx)\n",
    "            rows.append(batch_h_list)\n",
    "            cols.append(batch_t_list)\n",
    "            values.append(batch_v_list)\n",
    "\n",
    "        rows = torch.cat(rows)\n",
    "        cols = torch.cat(cols)\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        indices = torch.stack([rows, cols])\n",
    "        shape = self.A_in.shape\n",
    "        A_in = torch.sparse.FloatTensor(indices, values, torch.Size(shape))\n",
    "\n",
    "        A_in = torch.sparse.softmax(A_in.cpu(), dim=1)\n",
    "        self.A_in.data = A_in.to(device)\n",
    "\n",
    "    # Calculate scores for user-item pairs\n",
    "    def calc_score(self, user_ids, item_ids):\n",
    "        all_embed = self.calc_cf_embeddings()\n",
    "        user_embed = all_embed[user_ids]\n",
    "        item_embed = all_embed[item_ids]\n",
    "\n",
    "        cf_score = torch.matmul(user_embed, item_embed.transpose(0, 1))\n",
    "        return cf_score\n",
    "\n",
    "    # Main forward function with different modes\n",
    "    def forward(self, *input, mode):\n",
    "        if mode == 'train_cf':\n",
    "            return self.calc_cf_loss(*input)\n",
    "        if mode == 'train_kg':\n",
    "            return self.calc_kg_loss(*input)\n",
    "        if mode == 'update_att':\n",
    "            return self.update_attention(*input)\n",
    "        if mode == 'predict':\n",
    "            return self.calc_score(*input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e19cb6",
   "metadata": {},
   "source": [
    "## 2.2. Log_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "82152e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import csv\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Function to create a unique log ID by incrementing a counter\n",
    "def create_log_id(dir_path):\n",
    "    # Initialize the log count at 0\n",
    "    log_count = 0\n",
    "    # Create a file path with 'log0.log', 'log1.log', etc., until a unique file name is found\n",
    "    file_path = os.path.join(dir_path, 'log{:d}.log'.format(log_count))\n",
    "    while os.path.exists(file_path):\n",
    "        # Increment log_count and generate a new file path\n",
    "        log_count += 1\n",
    "        file_path = os.path.join(dir_path, 'log{:d}.log'.format(log_count))\n",
    "    return log_count  # Return the unique log ID\n",
    "\n",
    "# Function to configure logging settings\n",
    "def logging_config(folder=None, name=None,\n",
    "                   level=logging.DEBUG,\n",
    "                   console_level=logging.DEBUG,\n",
    "                   no_console=True):\n",
    "\n",
    "    # Create the log folder if it does not exist\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # Clear any existing handlers from the root logger\n",
    "    for handler in logging.root.handlers:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.root.handlers = []  # Reset handlers to avoid duplicate logs\n",
    "\n",
    "    # Set the log file path using the provided folder and log name\n",
    "    logpath = os.path.join(folder, name + \".log\")\n",
    "    print(\"All logs will be saved to %s\" % logpath)\n",
    "\n",
    "    # Set the logging level for the root logger\n",
    "    logging.root.setLevel(level)\n",
    "\n",
    "    # Define the log format\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "    # Create a file handler for logging to a file\n",
    "    logfile = logging.FileHandler(logpath)\n",
    "    logfile.setLevel(level)           # Set the level for the file handler\n",
    "    logfile.setFormatter(formatter)   # Apply the format to the file handler\n",
    "    logging.root.addHandler(logfile)  # Add the file handler to the root logger\n",
    "\n",
    "    # Optionally add a console handler to also log to the console\n",
    "    if not no_console:\n",
    "        logconsole = logging.StreamHandler()  # Create a stream handler for console output\n",
    "        logconsole.setLevel(console_level)    # Set the level for console logging\n",
    "        logconsole.setFormatter(formatter)    # Apply the format to the console handler\n",
    "        logging.root.addHandler(logconsole)   # Add the console handler to the root logger\n",
    "\n",
    "    return folder  # Return the log folder path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8e804",
   "metadata": {},
   "source": [
    "## 2.3. Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c6a65bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, log_loss, mean_squared_error\n",
    "\n",
    "# Function to calculate recall for a single example at a given cutoff k\n",
    "def calc_recall(rank, ground_truth, k):\n",
    "    \"\"\"\n",
    "    rank: list of predicted ranked items\n",
    "    ground_truth: list of true items\n",
    "    k: top-k items to consider\n",
    "    \"\"\"\n",
    "    return len(set(rank[:k]) & set(ground_truth)) / float(len(set(ground_truth)))\n",
    "\n",
    "# Function to calculate precision at k for a single user\n",
    "def precision_at_k(hit, k):\n",
    "    \"\"\"\n",
    "    hit: list of binary values indicating whether top-k items were relevant (1) or not (0)\n",
    "    \"\"\"\n",
    "    hit = np.asarray(hit)[:k]  # Take the first k elements\n",
    "    return np.mean(hit)  # Compute mean precision\n",
    "\n",
    "# Function to calculate precision at k for a batch of users\n",
    "def precision_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    hits: 2D array with rows representing users, columns binary (0 / 1) indicating relevance\n",
    "    \"\"\"\n",
    "    res = hits[:, :k].mean(axis=1)  # Calculate mean precision across users\n",
    "    return res\n",
    "\n",
    "# Function to calculate average precision\n",
    "def average_precision(hit, cut):\n",
    "    \"\"\"\n",
    "    hit: list of binary values indicating relevant items\n",
    "    cut: maximum position to consider for precision\n",
    "    \"\"\"\n",
    "    hit = np.asarray(hit)\n",
    "    precisions = [precision_at_k(hit, k + 1) for k in range(cut) if len(hit) >= k]\n",
    "    if not precisions:\n",
    "        return 0.0\n",
    "    return np.sum(precisions) / float(min(cut, np.sum(hit)))\n",
    "\n",
    "# Function to calculate Discounted Cumulative Gain (DCG) at k\n",
    "def dcg_at_k(rel, k):\n",
    "    \"\"\"\n",
    "    rel: list of relevance scores (binary or real) sorted by rank\n",
    "    \"\"\"\n",
    "    rel = np.asfarray(rel)[:k]\n",
    "    dcg = np.sum((2 ** rel - 1) / np.log2(np.arange(2, rel.size + 2)))  # Compute DCG\n",
    "    return dcg\n",
    "\n",
    "# Function to calculate Normalized Discounted Cumulative Gain (NDCG) at k\n",
    "def ndcg_at_k(rel, k):\n",
    "    \"\"\"\n",
    "    rel: list of relevance scores (binary or real)\n",
    "    \"\"\"\n",
    "    idcg = dcg_at_k(sorted(rel, reverse=True), k)  # Ideal DCG for normalization\n",
    "    if not idcg:\n",
    "        return 0.0\n",
    "    return dcg_at_k(rel, k) / idcg  # NDCG is DCG divided by IDCG\n",
    "\n",
    "# Function to calculate NDCG at k for a batch of users\n",
    "def ndcg_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    hits: 2D array of binary values indicating relevance\n",
    "    \"\"\"\n",
    "    hits_k = hits[:, :k]\n",
    "    dcg = np.sum((2 ** hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)  # Compute DCG\n",
    "\n",
    "    sorted_hits_k = np.flip(np.sort(hits), axis=1)[:, :k]  # Sort hits for ideal ranking\n",
    "    idcg = np.sum((2 ** sorted_hits_k - 1) / np.log2(np.arange(2, k + 2)), axis=1)\n",
    "\n",
    "    idcg[idcg == 0] = np.inf  # Handle cases where ideal DCG is zero\n",
    "    ndcg = (dcg / idcg)  # Compute NDCG\n",
    "    return ndcg\n",
    "\n",
    "# Function to calculate recall at k for a single user\n",
    "def recall_at_k(hit, k, all_pos_num):\n",
    "    \"\"\"\n",
    "    hit: list of binary values indicating relevant items\n",
    "    all_pos_num: total number of relevant items\n",
    "    \"\"\"\n",
    "    hit = np.asfarray(hit)[:k]\n",
    "    return np.sum(hit) / all_pos_num  # Compute recall\n",
    "\n",
    "# Function to calculate recall at k for a batch of users\n",
    "def recall_at_k_batch(hits, k):\n",
    "    \"\"\"\n",
    "    hits: 2D array of binary values indicating relevance\n",
    "    \"\"\"\n",
    "    res = (hits[:, :k].sum(axis=1) / hits.sum(axis=1))  # Recall per user\n",
    "    return res\n",
    "\n",
    "# Function to calculate F1 score from precision and recall\n",
    "def F1(pre, rec):\n",
    "    if pre + rec > 0:\n",
    "        return (2.0 * pre * rec) / (pre + rec)  # F1 score formula\n",
    "    else:\n",
    "        return 0.0\n",
    "\n",
    "# Function to calculate Area Under Curve (AUC) score\n",
    "def calc_auc(ground_truth, prediction):\n",
    "    try:\n",
    "        res = roc_auc_score(y_true=ground_truth, y_score=prediction)  # Compute AUC\n",
    "    except Exception:\n",
    "        res = 0.0  # Handle any exceptions\n",
    "    return res\n",
    "\n",
    "# Function to calculate log loss\n",
    "def logloss(ground_truth, prediction):\n",
    "    logloss = log_loss(np.asarray(ground_truth), np.asarray(prediction))  # Compute log loss\n",
    "    return logloss\n",
    "\n",
    "# Function to calculate various metrics at different cutoff points (Ks) for collaborative filtering\n",
    "def calc_metrics_at_k(cf_scores, train_user_dict, test_user_dict, user_ids, item_ids, Ks):\n",
    "    \"\"\"\n",
    "    cf_scores: 2D array of scores for each user-item pair\n",
    "    train_user_dict: dictionary of training items for each user\n",
    "    test_user_dict: dictionary of test items for each user\n",
    "    user_ids: list of user IDs\n",
    "    item_ids: list of item IDs\n",
    "    Ks: list of top-K values for evaluation\n",
    "    \"\"\"\n",
    "    # Create a binary matrix indicating test items\n",
    "    test_pos_item_binary = np.zeros([len(user_ids), len(item_ids)], dtype=np.float32)\n",
    "    for idx, u in enumerate(user_ids):\n",
    "        train_pos_item_list = train_user_dict.get(u, [])\n",
    "        test_pos_item_list = test_user_dict.get(u, [])\n",
    "        cf_scores[idx][train_pos_item_list] = -np.inf  # Exclude training items from ranking\n",
    "        test_pos_item_binary[idx][test_pos_item_list] = 1  # Mark test items\n",
    "\n",
    "    try:\n",
    "        _, rank_indices = torch.sort(cf_scores.cuda(), descending=True)  # Sort in descending order on GPU\n",
    "    except:\n",
    "        _, rank_indices = torch.sort(cf_scores, descending=True)  # Fallback to CPU sorting\n",
    "    rank_indices = rank_indices.cpu()  # Move result back to CPU\n",
    "\n",
    "    # Generate binary hit matrix based on sorted indices\n",
    "    binary_hit = []\n",
    "    for i in range(len(user_ids)):\n",
    "        binary_hit.append(test_pos_item_binary[i][rank_indices[i]])\n",
    "    binary_hit = np.array(binary_hit, dtype=np.float32)\n",
    "\n",
    "    # Calculate precision, recall, and NDCG metrics for each k in Ks\n",
    "    metrics_dict = {}\n",
    "    for k in Ks:\n",
    "        metrics_dict[k] = {}\n",
    "        metrics_dict[k]['precision'] = precision_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['recall'] = recall_at_k_batch(binary_hit, k)\n",
    "        metrics_dict[k]['ndcg'] = ndcg_at_k_batch(binary_hit, k)\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a54465d",
   "metadata": {},
   "source": [
    "## 2.4. Model_helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ae0d231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "\n",
    "# Function to determine early stopping based on recall scores\n",
    "def early_stopping(recall_list, stopping_steps):\n",
    "    # Find the maximum recall and its corresponding step\n",
    "    best_recall = max(recall_list)\n",
    "    best_step = recall_list.index(best_recall)\n",
    "    \n",
    "    # Check if the number of steps since the best step exceeds the stopping criterion\n",
    "    if len(recall_list) - best_step - 1 >= stopping_steps:\n",
    "        should_stop = True  # Set flag to stop training\n",
    "    else:\n",
    "        should_stop = False  # Continue training\n",
    "    return best_recall, should_stop  # Return best recall and stopping decision\n",
    "\n",
    "# Function to save model checkpoint\n",
    "def save_model(model, model_dir, current_epoch, last_best_epoch=None):\n",
    "    # Create the model directory if it doesn't exist\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "        \n",
    "    # Define the file path for saving the model at the current epoch\n",
    "    model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(current_epoch))\n",
    "    # Save the model state dictionary and current epoch in a .pth file\n",
    "    torch.save({'model_state_dict': model.state_dict(), 'epoch': current_epoch}, model_state_file)\n",
    "\n",
    "    # Remove the previous best model file if applicable\n",
    "    if last_best_epoch is not None and current_epoch != last_best_epoch:\n",
    "        old_model_state_file = os.path.join(model_dir, 'model_epoch{}.pth'.format(last_best_epoch))\n",
    "        # Delete the old model file if it exists\n",
    "        if os.path.exists(old_model_state_file):\n",
    "            os.system('rm {}'.format(old_model_state_file))  # Remove the file from the system\n",
    "\n",
    "# Function to load model checkpoint\n",
    "def load_model(model, model_path):\n",
    "    # Load the checkpoint from the specified path, mapping to CPU to ensure compatibility\n",
    "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "    # Load the saved model state dictionary into the model\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # Set the model to evaluation mode (for inference)\n",
    "    model.eval()\n",
    "    return model  # Return the loaded model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5c7b4",
   "metadata": {},
   "source": [
    "## 2.5. Loader_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1f0f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Base class for loading and processing data for recommendation models\n",
    "class DataLoaderBase(object):\n",
    "\n",
    "    # Initialize the DataLoader with paths, data files, and configuration\n",
    "    def __init__(self, args, logging):\n",
    "        self.args = args\n",
    "        self.data_name = args.data_name\n",
    "        self.use_pretrain = args.use_pretrain\n",
    "        self.pretrain_embedding_dir = args.pretrain_embedding_dir\n",
    "\n",
    "        # Set up data paths for training, testing, and knowledge graph files\n",
    "        self.data_dir = os.path.join(args.data_dir, args.data_name)\n",
    "        self.train_file = os.path.join(self.data_dir, 'train.txt')\n",
    "        self.test_file = os.path.join(self.data_dir, 'test.txt')\n",
    "        self.kg_file = os.path.join(self.data_dir, \"kg_final.txt\")\n",
    "\n",
    "        # Load collaborative filtering (CF) data and user-item interactions\n",
    "        self.cf_train_data, self.train_user_dict = self.load_cf(self.train_file)\n",
    "        self.cf_test_data, self.test_user_dict = self.load_cf(self.test_file)\n",
    "        self.statistic_cf()  # Calculate statistics on users and items\n",
    "\n",
    "        # Load pre-trained embeddings if required\n",
    "        if self.use_pretrain == 1:\n",
    "            self.load_pretrained_data()\n",
    "\n",
    "    # Load collaborative filtering data from a file\n",
    "    def load_cf(self, filename):\n",
    "        user = []\n",
    "        item = []\n",
    "        user_dict = dict()\n",
    "\n",
    "        # Read each line from the file representing user-item interactions\n",
    "        lines = open(filename, 'r').readlines()\n",
    "        for l in lines:\n",
    "            tmp = l.strip()\n",
    "            inter = [int(i) for i in tmp.split()]  # Convert to integers\n",
    "\n",
    "            if len(inter) > 1:\n",
    "                user_id, item_ids = inter[0], inter[1:]  # Extract user and item IDs\n",
    "                item_ids = list(set(item_ids))  # Remove duplicate items\n",
    "\n",
    "                # Append items and populate the user dictionary\n",
    "                for item_id in item_ids:\n",
    "                    user.append(user_id)\n",
    "                    item.append(item_id)\n",
    "                user_dict[user_id] = item_ids\n",
    "\n",
    "        # Convert lists to numpy arrays\n",
    "        user = np.array(user, dtype=np.int32)\n",
    "        item = np.array(item, dtype=np.int32)\n",
    "        return (user, item), user_dict\n",
    "\n",
    "    # Calculate statistics on the number of users, items, and interactions\n",
    "    def statistic_cf(self):\n",
    "        self.n_users = max(max(self.cf_train_data[0]), max(self.cf_test_data[0])) + 1\n",
    "        self.n_items = max(max(self.cf_train_data[1]), max(self.cf_test_data[1])) + 1\n",
    "        self.n_cf_train = len(self.cf_train_data[0])\n",
    "        self.n_cf_test = len(self.cf_test_data[0])\n",
    "\n",
    "    # Load knowledge graph (KG) data\n",
    "    def load_kg(self, filename):\n",
    "        kg_data = pd.read_csv(filename, sep=' ', names=['h', 'r', 't'], engine='python')  # Load head-relation-tail data\n",
    "        kg_data = kg_data.drop_duplicates()  # Remove duplicate triples\n",
    "        return kg_data\n",
    "\n",
    "    # Sample positive items for a user\n",
    "    def sample_pos_items_for_u(self, user_dict, user_id, n_sample_pos_items):\n",
    "        pos_items = user_dict[user_id]  # Get the list of positive items\n",
    "        n_pos_items = len(pos_items)\n",
    "\n",
    "        sample_pos_items = []\n",
    "        while True:\n",
    "            if len(sample_pos_items) == n_sample_pos_items:\n",
    "                break\n",
    "\n",
    "            pos_item_idx = np.random.randint(low=0, high=n_pos_items, size=1)[0]\n",
    "            pos_item_id = pos_items[pos_item_idx]\n",
    "            if pos_item_id not in sample_pos_items:\n",
    "                sample_pos_items.append(pos_item_id)\n",
    "        return sample_pos_items\n",
    "\n",
    "    # Sample negative items for a user\n",
    "    def sample_neg_items_for_u(self, user_dict, user_id, n_sample_neg_items):\n",
    "        pos_items = user_dict[user_id]  # Get the list of positive items\n",
    "\n",
    "        sample_neg_items = []\n",
    "        while True:\n",
    "            if len(sample_neg_items) == n_sample_neg_items:\n",
    "                break\n",
    "\n",
    "            neg_item_id = np.random.randint(low=0, high=self.n_items, size=1)[0]\n",
    "            if neg_item_id not in pos_items and neg_item_id not in sample_neg_items:\n",
    "                sample_neg_items.append(neg_item_id)\n",
    "        return sample_neg_items\n",
    "\n",
    "    # Generate a batch of user-item interactions for collaborative filtering\n",
    "    def generate_cf_batch(self, user_dict, batch_size):\n",
    "        exist_users = list(user_dict.keys())  # List of existing users\n",
    "        if batch_size <= len(exist_users):\n",
    "            batch_user = random.sample(exist_users, batch_size)  # Randomly sample users\n",
    "        else:\n",
    "            batch_user = [random.choice(exist_users) for _ in range(batch_size)]  # Sample with replacement if needed\n",
    "\n",
    "        batch_pos_item, batch_neg_item = [], []\n",
    "        for u in batch_user:\n",
    "            batch_pos_item += self.sample_pos_items_for_u(user_dict, u, 1)\n",
    "            batch_neg_item += self.sample_neg_items_for_u(user_dict, u, 1)\n",
    "\n",
    "        # Convert lists to torch tensors\n",
    "        batch_user = torch.LongTensor(batch_user)\n",
    "        batch_pos_item = torch.LongTensor(batch_pos_item)\n",
    "        batch_neg_item = torch.LongTensor(batch_neg_item)\n",
    "        return batch_user, batch_pos_item, batch_neg_item\n",
    "\n",
    "    # Sample positive triples for a given head entity in the knowledge graph\n",
    "    def sample_pos_triples_for_h(self, kg_dict, head, n_sample_pos_triples):\n",
    "        pos_triples = kg_dict[head]  # Get list of positive triples for head\n",
    "        n_pos_triples = len(pos_triples)\n",
    "\n",
    "        sample_relations, sample_pos_tails = [], []\n",
    "        while True:\n",
    "            if len(sample_relations) == n_sample_pos_triples:\n",
    "                break\n",
    "\n",
    "            pos_triple_idx = np.random.randint(low=0, high=n_pos_triples, size=1)[0]\n",
    "            tail = pos_triples[pos_triple_idx][0]\n",
    "            relation = pos_triples[pos_triple_idx][1]\n",
    "\n",
    "            if relation not in sample_relations and tail not in sample_pos_tails:\n",
    "                sample_relations.append(relation)\n",
    "                sample_pos_tails.append(tail)\n",
    "        return sample_relations, sample_pos_tails\n",
    "\n",
    "    # Sample negative triples for a given head entity and relation in the knowledge graph\n",
    "    def sample_neg_triples_for_h(self, kg_dict, head, relation, n_sample_neg_triples, highest_neg_idx):\n",
    "        pos_triples = kg_dict[head]  # Get positive triples for head\n",
    "\n",
    "        sample_neg_tails = []\n",
    "        while True:\n",
    "            if len(sample_neg_tails) == n_sample_neg_triples:\n",
    "                break\n",
    "\n",
    "            tail = np.random.randint(low=0, high=highest_neg_idx, size=1)[0]\n",
    "            if (tail, relation) not in pos_triples and tail not in sample_neg_tails:\n",
    "                sample_neg_tails.append(tail)\n",
    "        return sample_neg_tails\n",
    "\n",
    "    # Generate a batch of triples for knowledge graph training\n",
    "    def generate_kg_batch(self, kg_dict, batch_size, highest_neg_idx):\n",
    "        exist_heads = list(kg_dict.keys())  # List of existing heads in KG\n",
    "        if batch_size <= len(exist_heads):\n",
    "            batch_head = random.sample(exist_heads, batch_size)  # Randomly sample heads\n",
    "        else:\n",
    "            batch_head = [random.choice(exist_heads) for _ in range(batch_size)]  # Sample with replacement if needed\n",
    "\n",
    "        batch_relation, batch_pos_tail, batch_neg_tail = [], [], []\n",
    "        for h in batch_head:\n",
    "            relation, pos_tail = self.sample_pos_triples_for_h(kg_dict, h, 1)\n",
    "            batch_relation += relation\n",
    "            batch_pos_tail += pos_tail\n",
    "\n",
    "            neg_tail = self.sample_neg_triples_for_h(kg_dict, h, relation[0], 1, highest_neg_idx)\n",
    "            batch_neg_tail += neg_tail\n",
    "\n",
    "        # Convert lists to torch tensors\n",
    "        batch_head = torch.LongTensor(batch_head)\n",
    "        batch_relation = torch.LongTensor(batch_relation)\n",
    "        batch_pos_tail = torch.LongTensor(batch_pos_tail)\n",
    "        batch_neg_tail = torch.LongTensor(batch_neg_tail)\n",
    "        return batch_head, batch_relation, batch_pos_tail, batch_neg_tail\n",
    "\n",
    "    # Load pre-trained embeddings for users and items\n",
    "    def load_pretrained_data(self):\n",
    "        pre_model = 'mf'  # Specify model type for pre-trained embeddings\n",
    "        pretrain_path = '%s/%s/%s.npz' % (self.pretrain_embedding_dir, self.data_name, pre_model)  # Define path\n",
    "        pretrain_data = np.load(pretrain_path)  # Load pre-trained embeddings\n",
    "        self.user_pre_embed = pretrain_data['user_embed']\n",
    "        self.item_pre_embed = pretrain_data['item_embed']\n",
    "\n",
    "        # Validate embedding dimensions match the specified parameters\n",
    "        assert self.user_pre_embed.shape[0] == self.n_users\n",
    "        assert self.item_pre_embed.shape[0] == self.n_items\n",
    "        assert self.user_pre_embed.shape[1] == self.args.embed_dim\n",
    "        assert self.item_pre_embed.shape[1] == self.args.embed_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850efbc0",
   "metadata": {},
   "source": [
    "## 2.6. Loader_KGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e65a78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# DataLoaderKGAT class, extending from DataLoaderBase for the KGAT model\n",
    "class DataLoaderKGAT(DataLoaderBase):\n",
    "\n",
    "    # Initialization function\n",
    "    def __init__(self, args, logging):\n",
    "        super().__init__(args, logging)  # Initialize DataLoaderBase class\n",
    "\n",
    "        # Define number of users and entities based on training and test data\n",
    "        self.n_users = max(self.cf_train_data[0].max(), self.cf_test_data[0].max()) + 1\n",
    "        self.n_entities = max(self.cf_train_data[1].max(), self.cf_test_data[1].max()) + 1\n",
    "        # Set batch sizes for collaborative filtering, KG, and testing\n",
    "        self.cf_batch_size = args.cf_batch_size\n",
    "        self.kg_batch_size = args.kg_batch_size\n",
    "        self.test_batch_size = args.test_batch_size\n",
    "\n",
    "        # Load KG data and construct the required structures\n",
    "        kg_data = self.load_kg(self.kg_file)\n",
    "        self.construct_data(kg_data)\n",
    "        self.print_info(logging)  # Log information about data\n",
    "\n",
    "        # Set the laplacian type and initialize adjacency and laplacian dictionaries\n",
    "        self.laplacian_type = args.laplacian_type\n",
    "        self.create_adjacency_dict()\n",
    "        self.create_laplacian_dict()\n",
    "\n",
    "    # Method to construct data structures from KG data\n",
    "    def construct_data(self, kg_data):\n",
    "        # Duplicate KG data and reverse the head-tail for inverse relationships\n",
    "        n_relations = max(kg_data['r']) + 1\n",
    "        inverse_kg_data = kg_data.copy()\n",
    "        inverse_kg_data = inverse_kg_data.rename({'h': 't', 't': 'h'}, axis='columns')\n",
    "        inverse_kg_data['r'] += n_relations  # Assign new relation IDs for inverse relations\n",
    "        kg_data = pd.concat([kg_data, inverse_kg_data], axis=0, ignore_index=True, sort=False)\n",
    "\n",
    "        # Offset relation IDs for user-item interactions\n",
    "        kg_data['r'] += 2\n",
    "        self.n_relations = max(kg_data['r']) + 1  # Calculate total number of relations\n",
    "        self.n_entities = max(max(kg_data['h']), max(kg_data['t'])) + 1  # Update entity count\n",
    "        self.n_users_entities = self.n_users + self.n_entities  # Total count for users and entities\n",
    "\n",
    "        # Remap user IDs to ensure distinct user and entity spaces\n",
    "        self.cf_train_data = (np.array(list(map(lambda d: d + self.n_entities, self.cf_train_data[0]))).astype(np.int32), \n",
    "                              self.cf_train_data[1].astype(np.int32))\n",
    "        self.cf_test_data = (np.array(list(map(lambda d: d + self.n_entities, self.cf_test_data[0]))).astype(np.int32), \n",
    "                             self.cf_test_data[1].astype(np.int32))\n",
    "\n",
    "        # Offset user IDs in training and test dictionaries\n",
    "        self.train_user_dict = {k + self.n_entities: np.unique(v).astype(np.int32) for k, v in self.train_user_dict.items()}\n",
    "        self.test_user_dict = {k + self.n_entities: np.unique(v).astype(np.int32) for k, v in self.test_user_dict.items()}\n",
    "\n",
    "        # Create interaction data in KG format for training\n",
    "        cf2kg_train_data = pd.DataFrame(np.zeros((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        cf2kg_train_data['h'] = self.cf_train_data[0]\n",
    "        cf2kg_train_data['t'] = self.cf_train_data[1]\n",
    "\n",
    "        # Inverse interactions for undirected connections\n",
    "        inverse_cf2kg_train_data = pd.DataFrame(np.ones((self.n_cf_train, 3), dtype=np.int32), columns=['h', 'r', 't'])\n",
    "        inverse_cf2kg_train_data['h'] = self.cf_train_data[1]\n",
    "        inverse_cf2kg_train_data['t'] = self.cf_train_data[0]\n",
    "\n",
    "        # Concatenate all KG and CF data into a single KG training set\n",
    "        self.kg_train_data = pd.concat([kg_data, cf2kg_train_data, inverse_cf2kg_train_data], ignore_index=True)\n",
    "        self.n_kg_train = len(self.kg_train_data)  # Total KG training samples\n",
    "\n",
    "        # Construct dictionaries and lists for head, tail, and relation mapping\n",
    "        h_list, t_list, r_list = [], [], []\n",
    "        self.train_kg_dict = collections.defaultdict(list)\n",
    "        self.train_relation_dict = collections.defaultdict(list)\n",
    "\n",
    "        # Populate head, relation, and tail lists and dictionaries\n",
    "        for row in self.kg_train_data.iterrows():\n",
    "            h, r, t = row[1]\n",
    "            h_list.append(h)\n",
    "            t_list.append(t)\n",
    "            r_list.append(r)\n",
    "\n",
    "            self.train_kg_dict[h].append((t, r))\n",
    "            self.train_relation_dict[r].append((h, t))\n",
    "\n",
    "        # Convert lists to PyTorch tensors\n",
    "        self.h_list = torch.LongTensor(h_list)\n",
    "        self.t_list = torch.LongTensor(t_list)\n",
    "        self.r_list = torch.LongTensor(r_list)\n",
    "\n",
    "    # Convert a sparse matrix in COO format to a sparse PyTorch tensor\n",
    "    def convert_coo2tensor(self, coo):\n",
    "        values = coo.data  # Non-zero values of COO matrix\n",
    "        indices = np.vstack((coo.row, coo.col))  # Row and column indices\n",
    "\n",
    "        i = torch.LongTensor(indices)  # Convert indices to PyTorch tensor\n",
    "        v = torch.FloatTensor(values)  # Convert values to PyTorch tensor\n",
    "        shape = coo.shape\n",
    "        return torch.sparse.FloatTensor(i, v, torch.Size(shape))\n",
    "\n",
    "    # Create adjacency dictionary for each relation type\n",
    "    def create_adjacency_dict(self):\n",
    "        self.adjacency_dict = {}\n",
    "        for r, ht_list in self.train_relation_dict.items():\n",
    "            rows = [e[0] for e in ht_list]\n",
    "            cols = [e[1] for e in ht_list]\n",
    "            vals = [1] * len(rows)  # Use 1 for all adjacency values\n",
    "            adj = sp.coo_matrix((vals, (rows, cols)), shape=(self.n_users_entities, self.n_users_entities))\n",
    "            self.adjacency_dict[r] = adj  # Store adjacency matrix for each relation\n",
    "\n",
    "    # Create Laplacian matrices based on chosen normalization (symmetric or random walk)\n",
    "    def create_laplacian_dict(self):\n",
    "        \n",
    "        # Symmetric normalization of Laplacian matrix\n",
    "        def symmetric_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "            d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0  # Handle inf values\n",
    "            d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "\n",
    "            norm_adj = d_mat_inv_sqrt.dot(adj).dot(d_mat_inv_sqrt)  # Symmetric normalization\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        # Random walk normalization of Laplacian matrix\n",
    "        def random_walk_norm_lap(adj):\n",
    "            rowsum = np.array(adj.sum(axis=1))\n",
    "\n",
    "            d_inv = np.power(rowsum, -1.0).flatten()\n",
    "            d_inv[np.isinf(d_inv)] = 0  # Handle inf values\n",
    "            d_mat_inv = sp.diags(d_inv)\n",
    "\n",
    "            norm_adj = d_mat_inv.dot(adj)  # Random walk normalization\n",
    "            return norm_adj.tocoo()\n",
    "\n",
    "        # Choose normalization function based on laplacian type\n",
    "        if self.laplacian_type == 'symmetric':\n",
    "            norm_lap_func = symmetric_norm_lap\n",
    "        elif self.laplacian_type == 'random-walk':\n",
    "            norm_lap_func = random_walk_norm_lap\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # Create Laplacian matrix for each relation\n",
    "        self.laplacian_dict = {}\n",
    "        for r, adj in self.adjacency_dict.items():\n",
    "            self.laplacian_dict[r] = norm_lap_func(adj)\n",
    "\n",
    "        # Aggregate all Laplacians and convert to PyTorch tensor\n",
    "        A_in = sum(self.laplacian_dict.values())\n",
    "        self.A_in = self.convert_coo2tensor(A_in.tocoo())\n",
    "\n",
    "    # Log information about the dataset\n",
    "    def print_info(self, logging):\n",
    "        logging.info('n_users:           %d' % self.n_users)\n",
    "        logging.info('n_items:           %d' % self.n_items)\n",
    "        logging.info('n_entities:        %d' % self.n_entities)\n",
    "        logging.info('n_users_entities:  %d' % self.n_users_entities)\n",
    "        logging.info('n_relations:       %d' % self.n_relations)\n",
    "\n",
    "        logging.info('n_h_list:          %d' % len(self.h_list))\n",
    "        logging.info('n_t_list:          %d' % len(self.t_list))\n",
    "        logging.info('n_r_list:          %d' % len(self.r_list))\n",
    "\n",
    "        logging.info('n_cf_train:        %d' % self.n_cf_train)\n",
    "        logging.info('n_cf_test:         %d' % self.n_cf_test)\n",
    "\n",
    "        logging.info('n_kg_train:        %d' % self.n_kg_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79abcb6",
   "metadata": {},
   "source": [
    "## 2.7. Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef68e757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "def parse_kgat_args():\n",
    "    parser = argparse.ArgumentParser(description=\"Run KGAT.\")\n",
    "\n",
    "    parser.add_argument('--seed', type=int, default=2024,\n",
    "                        help='Random seed.')\n",
    "\n",
    "    parser.add_argument('--data_name', nargs='?', default='exchange-students',\n",
    "                        help='Choose a dataset from {yelp2018, last-fm, amazon-book}')\n",
    "    parser.add_argument('--data_dir', nargs='?', default='/Users/gayeonlee/Documents/2024/rec_system/kgat_data/',\n",
    "                        help='Input data path.')\n",
    "\n",
    "    parser.add_argument('--use_pretrain', type=int, default=0,\n",
    "                        help='0: No pretrain, 1: Pretrain with the learned embeddings, 2: Pretrain with stored model.')\n",
    "    parser.add_argument('--pretrain_embedding_dir', nargs='?', default='/Users/gayeonlee/Documents/2024/rec_system/',\n",
    "                        help='Path of learned embeddings.')\n",
    "    parser.add_argument('--pretrain_model_path', nargs='?', default='trained_model/model.pth',\n",
    "                        help='Path of stored model.')\n",
    "\n",
    "    parser.add_argument('--cf_batch_size', type=int, default=64,\n",
    "                        help='CF batch size.')\n",
    "    parser.add_argument('--kg_batch_size', type=int, default=128,\n",
    "                        help='KG batch size.')\n",
    "    parser.add_argument('--test_batch_size', type=int, default=100,\n",
    "                        help='Test batch size (the user number to test every batch).')\n",
    "\n",
    "    parser.add_argument('--embed_dim', type=int, default=64,\n",
    "                        help='User / entity Embedding size.')\n",
    "    parser.add_argument('--relation_dim', type=int, default=64,\n",
    "                        help='Relation Embedding size.')\n",
    "\n",
    "    parser.add_argument('--laplacian_type', type=str, default='random-walk',\n",
    "                        help='Specify the type of the adjacency (laplacian) matrix from {symmetric, random-walk}.')\n",
    "    parser.add_argument('--aggregation_type', type=str, default='bi-interaction',\n",
    "                        help='Specify the type of the aggregation layer from {gcn, graphsage, bi-interaction}.')\n",
    "    parser.add_argument('--conv_dim_list', nargs='?', default='[64, 32, 16]',\n",
    "                        help='Output sizes of every aggregation layer.')\n",
    "    parser.add_argument('--mess_dropout', nargs='?', default='[0.1, 0.1, 0.1]',\n",
    "                        help='Dropout probability w.r.t. message dropout for each deep layer. 0: no dropout.')\n",
    "\n",
    "    parser.add_argument('--kg_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating KG l2 loss.')\n",
    "    parser.add_argument('--cf_l2loss_lambda', type=float, default=1e-5,\n",
    "                        help='Lambda when calculating CF l2 loss.')\n",
    "\n",
    "    parser.add_argument('--lr', type=float, default=0.0001,\n",
    "                        help='Learning rate.')\n",
    "    parser.add_argument('--n_epoch', type=int, default=3,\n",
    "                        help='Number of epoch.')\n",
    "    parser.add_argument('--stopping_steps', type=int, default=1,\n",
    "                        help='Number of epoch for early stopping')\n",
    "\n",
    "    parser.add_argument('--cf_print_every', type=int, default=3,\n",
    "                        help='Iter interval of printing CF loss.')\n",
    "    parser.add_argument('--kg_print_every', type=int, default=3,\n",
    "                        help='Iter interval of printing KG loss.')\n",
    "    parser.add_argument('--evaluate_every', type=int, default=3,\n",
    "                        help='Epoch interval of evaluating CF.')\n",
    "\n",
    "    parser.add_argument('--Ks', nargs='?', default='[10, 12, 14, 16, 20]',\n",
    "                        help='Calculate metric@K when evaluating.')\n",
    "\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "    save_dir = 'trained_model/KGAT/{}/embed-dim{}_relation-dim{}_{}_{}_{}_lr{}_pretrain{}/'.format(\n",
    "        args.data_name, args.embed_dim, args.relation_dim, args.laplacian_type, args.aggregation_type,\n",
    "        '-'.join([str(i) for i in eval(args.conv_dim_list)]), args.lr, args.use_pretrain)\n",
    "    args.save_dir = save_dir\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b36f9",
   "metadata": {},
   "source": [
    "## 2.8. Train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b855f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 04:10:24,792 - root - INFO - Namespace(seed=2024, data_name='exchange-students', data_dir='/Users/gayeonlee/Documents/2024/rec_system/kgat_data/', use_pretrain=0, pretrain_embedding_dir='/Users/gayeonlee/Documents/2024/rec_system/', pretrain_model_path='trained_model/model.pth', cf_batch_size=64, kg_batch_size=128, test_batch_size=100, embed_dim=64, relation_dim=64, laplacian_type='random-walk', aggregation_type='bi-interaction', conv_dim_list='[64, 32, 16]', mess_dropout='[0.1, 0.1, 0.1]', kg_l2loss_lambda=1e-05, cf_l2loss_lambda=1e-05, lr=0.0001, n_epoch=3, stopping_steps=1, cf_print_every=3, kg_print_every=3, evaluate_every=3, Ks='[10, 12, 14, 16, 20]', save_dir='trained_model/KGAT/exchange-students/embed-dim64_relation-dim64_random-walk_bi-interaction_64-32-16_lr0.0001_pretrain0/')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All logs will be saved to trained_model/KGAT/exchange-students/embed-dim64_relation-dim64_random-walk_bi-interaction_64-32-16_lr0.0001_pretrain0/log4.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-10 04:10:25,116 - root - INFO - n_users:           2309\n",
      "2024-12-10 04:10:25,116 - root - INFO - n_items:           196\n",
      "2024-12-10 04:10:25,117 - root - INFO - n_entities:        2309\n",
      "2024-12-10 04:10:25,117 - root - INFO - n_users_entities:  4618\n",
      "2024-12-10 04:10:25,117 - root - INFO - n_relations:       30\n",
      "2024-12-10 04:10:25,118 - root - INFO - n_h_list:          27520\n",
      "2024-12-10 04:10:25,118 - root - INFO - n_t_list:          27520\n",
      "2024-12-10 04:10:25,118 - root - INFO - n_r_list:          27520\n",
      "2024-12-10 04:10:25,118 - root - INFO - n_cf_train:        1847\n",
      "2024-12-10 04:10:25,118 - root - INFO - n_cf_test:         462\n",
      "2024-12-10 04:10:25,119 - root - INFO - n_kg_train:        27520\n",
      "/var/folders/4s/3r4lgj2968jckh9ss37g6d1w0000gn/T/ipykernel_56384/3746279008.py:132: RuntimeWarning: divide by zero encountered in power\n",
      "  d_inv = np.power(rowsum, -1.0).flatten()\n",
      "2024-12-10 04:10:25,152 - root - INFO - KGAT(\n",
      "  (user_entity_embed): Embedding(4618, 64)\n",
      "  (relation_embed): Embedding(30, 64)\n",
      "  (entity_user_embed): Embedding(4618, 64)\n",
      "  (aggregator_layers): ModuleList(\n",
      "    (0): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (linear2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    )\n",
      "    (1): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (linear2): Linear(in_features=64, out_features=32, bias=True)\n",
      "    )\n",
      "    (2): Aggregator(\n",
      "      (message_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (activation): LeakyReLU(negative_slope=0.01)\n",
      "      (linear1): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (linear2): Linear(in_features=32, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2024-12-10 04:10:25,192 - root - INFO - CF Training: Epoch 0001 Iter 0003 / 0029 | Time 0.0s | Iter Loss 0.6745 | Iter Mean Loss 0.6728\n",
      "2024-12-10 04:10:25,229 - root - INFO - CF Training: Epoch 0001 Iter 0006 / 0029 | Time 0.0s | Iter Loss 0.6564 | Iter Mean Loss 0.6669\n",
      "2024-12-10 04:10:25,266 - root - INFO - CF Training: Epoch 0001 Iter 0009 / 0029 | Time 0.0s | Iter Loss 0.6697 | Iter Mean Loss 0.6685\n",
      "2024-12-10 04:10:25,316 - root - INFO - CF Training: Epoch 0001 Iter 0012 / 0029 | Time 0.0s | Iter Loss 0.6789 | Iter Mean Loss 0.6727\n",
      "2024-12-10 04:10:25,350 - root - INFO - CF Training: Epoch 0001 Iter 0015 / 0029 | Time 0.0s | Iter Loss 0.6794 | Iter Mean Loss 0.6725\n",
      "2024-12-10 04:10:25,390 - root - INFO - CF Training: Epoch 0001 Iter 0018 / 0029 | Time 0.0s | Iter Loss 0.6618 | Iter Mean Loss 0.6722\n",
      "2024-12-10 04:10:25,427 - root - INFO - CF Training: Epoch 0001 Iter 0021 / 0029 | Time 0.0s | Iter Loss 0.6630 | Iter Mean Loss 0.6721\n",
      "2024-12-10 04:10:25,468 - root - INFO - CF Training: Epoch 0001 Iter 0024 / 0029 | Time 0.0s | Iter Loss 0.6590 | Iter Mean Loss 0.6718\n",
      "2024-12-10 04:10:25,547 - root - INFO - CF Training: Epoch 0001 Iter 0027 / 0029 | Time 0.1s | Iter Loss 0.6572 | Iter Mean Loss 0.6710\n",
      "2024-12-10 04:10:25,573 - root - INFO - CF Training: Epoch 0001 Total Iter 0029 | Total Time 0.4s | Iter Mean Loss 0.6702\n",
      "2024-12-10 04:10:25,587 - root - INFO - KG Training: Epoch 0001 Iter 0003 / 0216 | Time 0.0s | Iter Loss 0.6931 | Iter Mean Loss 0.6931\n",
      "2024-12-10 04:10:25,603 - root - INFO - KG Training: Epoch 0001 Iter 0006 / 0216 | Time 0.0s | Iter Loss 0.6919 | Iter Mean Loss 0.6926\n",
      "2024-12-10 04:10:25,619 - root - INFO - KG Training: Epoch 0001 Iter 0009 / 0216 | Time 0.0s | Iter Loss 0.6905 | Iter Mean Loss 0.6921\n",
      "2024-12-10 04:10:25,635 - root - INFO - KG Training: Epoch 0001 Iter 0012 / 0216 | Time 0.0s | Iter Loss 0.6905 | Iter Mean Loss 0.6919\n",
      "2024-12-10 04:10:25,651 - root - INFO - KG Training: Epoch 0001 Iter 0015 / 0216 | Time 0.0s | Iter Loss 0.6899 | Iter Mean Loss 0.6915\n",
      "2024-12-10 04:10:25,667 - root - INFO - KG Training: Epoch 0001 Iter 0018 / 0216 | Time 0.0s | Iter Loss 0.6886 | Iter Mean Loss 0.6910\n",
      "2024-12-10 04:10:25,683 - root - INFO - KG Training: Epoch 0001 Iter 0021 / 0216 | Time 0.0s | Iter Loss 0.6881 | Iter Mean Loss 0.6907\n",
      "2024-12-10 04:10:25,699 - root - INFO - KG Training: Epoch 0001 Iter 0024 / 0216 | Time 0.0s | Iter Loss 0.6875 | Iter Mean Loss 0.6903\n",
      "2024-12-10 04:10:25,714 - root - INFO - KG Training: Epoch 0001 Iter 0027 / 0216 | Time 0.0s | Iter Loss 0.6861 | Iter Mean Loss 0.6899\n",
      "2024-12-10 04:10:25,731 - root - INFO - KG Training: Epoch 0001 Iter 0030 / 0216 | Time 0.0s | Iter Loss 0.6863 | Iter Mean Loss 0.6895\n",
      "2024-12-10 04:10:25,751 - root - INFO - KG Training: Epoch 0001 Iter 0033 / 0216 | Time 0.0s | Iter Loss 0.6846 | Iter Mean Loss 0.6891\n",
      "2024-12-10 04:10:25,769 - root - INFO - KG Training: Epoch 0001 Iter 0036 / 0216 | Time 0.0s | Iter Loss 0.6838 | Iter Mean Loss 0.6887\n",
      "2024-12-10 04:10:25,788 - root - INFO - KG Training: Epoch 0001 Iter 0039 / 0216 | Time 0.0s | Iter Loss 0.6837 | Iter Mean Loss 0.6883\n",
      "2024-12-10 04:10:25,808 - root - INFO - KG Training: Epoch 0001 Iter 0042 / 0216 | Time 0.0s | Iter Loss 0.6813 | Iter Mean Loss 0.6878\n",
      "2024-12-10 04:10:25,827 - root - INFO - KG Training: Epoch 0001 Iter 0045 / 0216 | Time 0.0s | Iter Loss 0.6815 | Iter Mean Loss 0.6874\n",
      "2024-12-10 04:10:25,845 - root - INFO - KG Training: Epoch 0001 Iter 0048 / 0216 | Time 0.0s | Iter Loss 0.6795 | Iter Mean Loss 0.6869\n",
      "2024-12-10 04:10:25,865 - root - INFO - KG Training: Epoch 0001 Iter 0051 / 0216 | Time 0.0s | Iter Loss 0.6756 | Iter Mean Loss 0.6864\n",
      "2024-12-10 04:10:25,884 - root - INFO - KG Training: Epoch 0001 Iter 0054 / 0216 | Time 0.0s | Iter Loss 0.6808 | Iter Mean Loss 0.6860\n",
      "2024-12-10 04:10:25,904 - root - INFO - KG Training: Epoch 0001 Iter 0057 / 0216 | Time 0.0s | Iter Loss 0.6749 | Iter Mean Loss 0.6854\n",
      "2024-12-10 04:10:25,924 - root - INFO - KG Training: Epoch 0001 Iter 0060 / 0216 | Time 0.0s | Iter Loss 0.6734 | Iter Mean Loss 0.6848\n",
      "2024-12-10 04:10:25,943 - root - INFO - KG Training: Epoch 0001 Iter 0063 / 0216 | Time 0.0s | Iter Loss 0.6729 | Iter Mean Loss 0.6843\n",
      "2024-12-10 04:10:25,960 - root - INFO - KG Training: Epoch 0001 Iter 0066 / 0216 | Time 0.0s | Iter Loss 0.6685 | Iter Mean Loss 0.6836\n",
      "2024-12-10 04:10:25,978 - root - INFO - KG Training: Epoch 0001 Iter 0069 / 0216 | Time 0.0s | Iter Loss 0.6688 | Iter Mean Loss 0.6831\n",
      "2024-12-10 04:10:25,994 - root - INFO - KG Training: Epoch 0001 Iter 0072 / 0216 | Time 0.0s | Iter Loss 0.6680 | Iter Mean Loss 0.6824\n",
      "2024-12-10 04:10:26,035 - root - INFO - KG Training: Epoch 0001 Iter 0075 / 0216 | Time 0.0s | Iter Loss 0.6686 | Iter Mean Loss 0.6818\n",
      "2024-12-10 04:10:26,052 - root - INFO - KG Training: Epoch 0001 Iter 0078 / 0216 | Time 0.0s | Iter Loss 0.6630 | Iter Mean Loss 0.6811\n",
      "2024-12-10 04:10:26,066 - root - INFO - KG Training: Epoch 0001 Iter 0081 / 0216 | Time 0.0s | Iter Loss 0.6600 | Iter Mean Loss 0.6804\n",
      "2024-12-10 04:10:26,082 - root - INFO - KG Training: Epoch 0001 Iter 0084 / 0216 | Time 0.0s | Iter Loss 0.6606 | Iter Mean Loss 0.6796\n",
      "2024-12-10 04:10:26,096 - root - INFO - KG Training: Epoch 0001 Iter 0087 / 0216 | Time 0.0s | Iter Loss 0.6535 | Iter Mean Loss 0.6787\n",
      "2024-12-10 04:10:26,113 - root - INFO - KG Training: Epoch 0001 Iter 0090 / 0216 | Time 0.0s | Iter Loss 0.6581 | Iter Mean Loss 0.6780\n",
      "2024-12-10 04:10:26,135 - root - INFO - KG Training: Epoch 0001 Iter 0093 / 0216 | Time 0.0s | Iter Loss 0.6502 | Iter Mean Loss 0.6771\n",
      "2024-12-10 04:10:26,153 - root - INFO - KG Training: Epoch 0001 Iter 0096 / 0216 | Time 0.0s | Iter Loss 0.6534 | Iter Mean Loss 0.6763\n",
      "2024-12-10 04:10:26,172 - root - INFO - KG Training: Epoch 0001 Iter 0099 / 0216 | Time 0.0s | Iter Loss 0.6463 | Iter Mean Loss 0.6754\n",
      "2024-12-10 04:10:26,188 - root - INFO - KG Training: Epoch 0001 Iter 0102 / 0216 | Time 0.0s | Iter Loss 0.6372 | Iter Mean Loss 0.6745\n",
      "2024-12-10 04:10:26,203 - root - INFO - KG Training: Epoch 0001 Iter 0105 / 0216 | Time 0.0s | Iter Loss 0.6365 | Iter Mean Loss 0.6735\n",
      "2024-12-10 04:10:26,220 - root - INFO - KG Training: Epoch 0001 Iter 0108 / 0216 | Time 0.0s | Iter Loss 0.6329 | Iter Mean Loss 0.6724\n",
      "2024-12-10 04:10:26,235 - root - INFO - KG Training: Epoch 0001 Iter 0111 / 0216 | Time 0.0s | Iter Loss 0.6314 | Iter Mean Loss 0.6713\n",
      "2024-12-10 04:10:26,250 - root - INFO - KG Training: Epoch 0001 Iter 0114 / 0216 | Time 0.0s | Iter Loss 0.6250 | Iter Mean Loss 0.6703\n",
      "2024-12-10 04:10:26,267 - root - INFO - KG Training: Epoch 0001 Iter 0117 / 0216 | Time 0.0s | Iter Loss 0.6264 | Iter Mean Loss 0.6692\n",
      "2024-12-10 04:10:26,284 - root - INFO - KG Training: Epoch 0001 Iter 0120 / 0216 | Time 0.0s | Iter Loss 0.6186 | Iter Mean Loss 0.6680\n",
      "2024-12-10 04:10:26,299 - root - INFO - KG Training: Epoch 0001 Iter 0123 / 0216 | Time 0.0s | Iter Loss 0.6097 | Iter Mean Loss 0.6668\n",
      "2024-12-10 04:10:26,315 - root - INFO - KG Training: Epoch 0001 Iter 0126 / 0216 | Time 0.0s | Iter Loss 0.6201 | Iter Mean Loss 0.6657\n",
      "2024-12-10 04:10:26,330 - root - INFO - KG Training: Epoch 0001 Iter 0129 / 0216 | Time 0.0s | Iter Loss 0.6113 | Iter Mean Loss 0.6643\n",
      "2024-12-10 04:10:26,348 - root - INFO - KG Training: Epoch 0001 Iter 0132 / 0216 | Time 0.0s | Iter Loss 0.6063 | Iter Mean Loss 0.6631\n",
      "2024-12-10 04:10:26,364 - root - INFO - KG Training: Epoch 0001 Iter 0135 / 0216 | Time 0.0s | Iter Loss 0.5970 | Iter Mean Loss 0.6618\n",
      "2024-12-10 04:10:26,382 - root - INFO - KG Training: Epoch 0001 Iter 0138 / 0216 | Time 0.0s | Iter Loss 0.5980 | Iter Mean Loss 0.6604\n",
      "2024-12-10 04:10:26,397 - root - INFO - KG Training: Epoch 0001 Iter 0141 / 0216 | Time 0.0s | Iter Loss 0.5935 | Iter Mean Loss 0.6591\n",
      "2024-12-10 04:10:26,413 - root - INFO - KG Training: Epoch 0001 Iter 0144 / 0216 | Time 0.0s | Iter Loss 0.5825 | Iter Mean Loss 0.6575\n",
      "2024-12-10 04:10:26,433 - root - INFO - KG Training: Epoch 0001 Iter 0147 / 0216 | Time 0.0s | Iter Loss 0.5855 | Iter Mean Loss 0.6561\n",
      "2024-12-10 04:10:26,451 - root - INFO - KG Training: Epoch 0001 Iter 0150 / 0216 | Time 0.0s | Iter Loss 0.5754 | Iter Mean Loss 0.6546\n",
      "2024-12-10 04:10:26,469 - root - INFO - KG Training: Epoch 0001 Iter 0153 / 0216 | Time 0.0s | Iter Loss 0.5686 | Iter Mean Loss 0.6531\n",
      "2024-12-10 04:10:26,487 - root - INFO - KG Training: Epoch 0001 Iter 0156 / 0216 | Time 0.0s | Iter Loss 0.5733 | Iter Mean Loss 0.6516\n",
      "2024-12-10 04:10:26,505 - root - INFO - KG Training: Epoch 0001 Iter 0159 / 0216 | Time 0.0s | Iter Loss 0.5597 | Iter Mean Loss 0.6500\n",
      "2024-12-10 04:10:26,526 - root - INFO - KG Training: Epoch 0001 Iter 0162 / 0216 | Time 0.0s | Iter Loss 0.5690 | Iter Mean Loss 0.6485\n",
      "2024-12-10 04:10:26,547 - root - INFO - KG Training: Epoch 0001 Iter 0165 / 0216 | Time 0.0s | Iter Loss 0.5614 | Iter Mean Loss 0.6468\n",
      "2024-12-10 04:10:26,562 - root - INFO - KG Training: Epoch 0001 Iter 0168 / 0216 | Time 0.0s | Iter Loss 0.5566 | Iter Mean Loss 0.6452\n",
      "2024-12-10 04:10:26,579 - root - INFO - KG Training: Epoch 0001 Iter 0171 / 0216 | Time 0.0s | Iter Loss 0.5506 | Iter Mean Loss 0.6435\n",
      "2024-12-10 04:10:26,596 - root - INFO - KG Training: Epoch 0001 Iter 0174 / 0216 | Time 0.0s | Iter Loss 0.5413 | Iter Mean Loss 0.6418\n",
      "2024-12-10 04:10:26,611 - root - INFO - KG Training: Epoch 0001 Iter 0177 / 0216 | Time 0.0s | Iter Loss 0.5350 | Iter Mean Loss 0.6399\n",
      "2024-12-10 04:10:26,627 - root - INFO - KG Training: Epoch 0001 Iter 0180 / 0216 | Time 0.0s | Iter Loss 0.5243 | Iter Mean Loss 0.6381\n",
      "2024-12-10 04:10:26,642 - root - INFO - KG Training: Epoch 0001 Iter 0183 / 0216 | Time 0.0s | Iter Loss 0.5257 | Iter Mean Loss 0.6364\n",
      "2024-12-10 04:10:26,659 - root - INFO - KG Training: Epoch 0001 Iter 0186 / 0216 | Time 0.0s | Iter Loss 0.5192 | Iter Mean Loss 0.6345\n",
      "2024-12-10 04:10:26,676 - root - INFO - KG Training: Epoch 0001 Iter 0189 / 0216 | Time 0.0s | Iter Loss 0.5380 | Iter Mean Loss 0.6328\n",
      "2024-12-10 04:10:26,692 - root - INFO - KG Training: Epoch 0001 Iter 0192 / 0216 | Time 0.0s | Iter Loss 0.4970 | Iter Mean Loss 0.6308\n",
      "2024-12-10 04:10:26,709 - root - INFO - KG Training: Epoch 0001 Iter 0195 / 0216 | Time 0.0s | Iter Loss 0.4900 | Iter Mean Loss 0.6288\n",
      "2024-12-10 04:10:26,725 - root - INFO - KG Training: Epoch 0001 Iter 0198 / 0216 | Time 0.0s | Iter Loss 0.5036 | Iter Mean Loss 0.6268\n",
      "2024-12-10 04:10:26,740 - root - INFO - KG Training: Epoch 0001 Iter 0201 / 0216 | Time 0.0s | Iter Loss 0.5021 | Iter Mean Loss 0.6249\n",
      "2024-12-10 04:10:26,757 - root - INFO - KG Training: Epoch 0001 Iter 0204 / 0216 | Time 0.0s | Iter Loss 0.4851 | Iter Mean Loss 0.6229\n",
      "2024-12-10 04:10:26,772 - root - INFO - KG Training: Epoch 0001 Iter 0207 / 0216 | Time 0.0s | Iter Loss 0.4854 | Iter Mean Loss 0.6209\n",
      "2024-12-10 04:10:26,788 - root - INFO - KG Training: Epoch 0001 Iter 0210 / 0216 | Time 0.0s | Iter Loss 0.4761 | Iter Mean Loss 0.6189\n",
      "2024-12-10 04:10:26,804 - root - INFO - KG Training: Epoch 0001 Iter 0213 / 0216 | Time 0.0s | Iter Loss 0.4774 | Iter Mean Loss 0.6168\n",
      "2024-12-10 04:10:26,820 - root - INFO - KG Training: Epoch 0001 Iter 0216 / 0216 | Time 0.0s | Iter Loss 0.4604 | Iter Mean Loss 0.6148\n",
      "2024-12-10 04:10:26,821 - root - INFO - KG Training: Epoch 0001 Total Iter 0216 | Total Time 1.2s | Iter Mean Loss 0.6148\n",
      "2024-12-10 04:10:26,832 - root - INFO - Update Attention: Epoch 0001 | Total Time 0.0s\n",
      "2024-12-10 04:10:26,832 - root - INFO - CF + KG Training: Epoch 0001 | Total Time 1.7s\n",
      "2024-12-10 04:10:26,867 - root - INFO - CF Training: Epoch 0002 Iter 0003 / 0029 | Time 0.0s | Iter Loss 0.6358 | Iter Mean Loss 0.6559\n",
      "2024-12-10 04:10:26,899 - root - INFO - CF Training: Epoch 0002 Iter 0006 / 0029 | Time 0.0s | Iter Loss 0.6428 | Iter Mean Loss 0.6522\n",
      "2024-12-10 04:10:26,930 - root - INFO - CF Training: Epoch 0002 Iter 0009 / 0029 | Time 0.0s | Iter Loss 0.6475 | Iter Mean Loss 0.6510\n",
      "2024-12-10 04:10:26,963 - root - INFO - CF Training: Epoch 0002 Iter 0012 / 0029 | Time 0.0s | Iter Loss 0.6381 | Iter Mean Loss 0.6498\n",
      "2024-12-10 04:10:26,996 - root - INFO - CF Training: Epoch 0002 Iter 0015 / 0029 | Time 0.0s | Iter Loss 0.6380 | Iter Mean Loss 0.6480\n",
      "2024-12-10 04:10:27,028 - root - INFO - CF Training: Epoch 0002 Iter 0018 / 0029 | Time 0.0s | Iter Loss 0.6401 | Iter Mean Loss 0.6456\n",
      "2024-12-10 04:10:27,061 - root - INFO - CF Training: Epoch 0002 Iter 0021 / 0029 | Time 0.0s | Iter Loss 0.6501 | Iter Mean Loss 0.6446\n",
      "2024-12-10 04:10:27,095 - root - INFO - CF Training: Epoch 0002 Iter 0024 / 0029 | Time 0.0s | Iter Loss 0.6440 | Iter Mean Loss 0.6430\n",
      "2024-12-10 04:10:27,159 - root - INFO - CF Training: Epoch 0002 Iter 0027 / 0029 | Time 0.0s | Iter Loss 0.6380 | Iter Mean Loss 0.6417\n",
      "2024-12-10 04:10:27,181 - root - INFO - CF Training: Epoch 0002 Total Iter 0029 | Total Time 0.3s | Iter Mean Loss 0.6416\n",
      "2024-12-10 04:10:27,197 - root - INFO - KG Training: Epoch 0002 Iter 0003 / 0216 | Time 0.0s | Iter Loss 0.4561 | Iter Mean Loss 0.4615\n",
      "2024-12-10 04:10:27,213 - root - INFO - KG Training: Epoch 0002 Iter 0006 / 0216 | Time 0.0s | Iter Loss 0.4450 | Iter Mean Loss 0.4595\n",
      "2024-12-10 04:10:27,229 - root - INFO - KG Training: Epoch 0002 Iter 0009 / 0216 | Time 0.0s | Iter Loss 0.4461 | Iter Mean Loss 0.4560\n",
      "2024-12-10 04:10:27,242 - root - INFO - KG Training: Epoch 0002 Iter 0012 / 0216 | Time 0.0s | Iter Loss 0.4365 | Iter Mean Loss 0.4519\n",
      "2024-12-10 04:10:27,259 - root - INFO - KG Training: Epoch 0002 Iter 0015 / 0216 | Time 0.0s | Iter Loss 0.4315 | Iter Mean Loss 0.4483\n",
      "2024-12-10 04:10:27,276 - root - INFO - KG Training: Epoch 0002 Iter 0018 / 0216 | Time 0.0s | Iter Loss 0.4417 | Iter Mean Loss 0.4461\n",
      "2024-12-10 04:10:27,289 - root - INFO - KG Training: Epoch 0002 Iter 0021 / 0216 | Time 0.0s | Iter Loss 0.4166 | Iter Mean Loss 0.4425\n",
      "2024-12-10 04:10:27,304 - root - INFO - KG Training: Epoch 0002 Iter 0024 / 0216 | Time 0.0s | Iter Loss 0.4191 | Iter Mean Loss 0.4399\n",
      "2024-12-10 04:10:27,320 - root - INFO - KG Training: Epoch 0002 Iter 0027 / 0216 | Time 0.0s | Iter Loss 0.4202 | Iter Mean Loss 0.4372\n",
      "2024-12-10 04:10:27,335 - root - INFO - KG Training: Epoch 0002 Iter 0030 / 0216 | Time 0.0s | Iter Loss 0.4072 | Iter Mean Loss 0.4344\n",
      "2024-12-10 04:10:27,351 - root - INFO - KG Training: Epoch 0002 Iter 0033 / 0216 | Time 0.0s | Iter Loss 0.3987 | Iter Mean Loss 0.4318\n",
      "2024-12-10 04:10:27,367 - root - INFO - KG Training: Epoch 0002 Iter 0036 / 0216 | Time 0.0s | Iter Loss 0.3902 | Iter Mean Loss 0.4291\n",
      "2024-12-10 04:10:27,384 - root - INFO - KG Training: Epoch 0002 Iter 0039 / 0216 | Time 0.0s | Iter Loss 0.3814 | Iter Mean Loss 0.4261\n",
      "2024-12-10 04:10:27,401 - root - INFO - KG Training: Epoch 0002 Iter 0042 / 0216 | Time 0.0s | Iter Loss 0.4063 | Iter Mean Loss 0.4236\n",
      "2024-12-10 04:10:27,417 - root - INFO - KG Training: Epoch 0002 Iter 0045 / 0216 | Time 0.0s | Iter Loss 0.3794 | Iter Mean Loss 0.4212\n",
      "2024-12-10 04:10:27,433 - root - INFO - KG Training: Epoch 0002 Iter 0048 / 0216 | Time 0.0s | Iter Loss 0.3617 | Iter Mean Loss 0.4183\n",
      "2024-12-10 04:10:27,448 - root - INFO - KG Training: Epoch 0002 Iter 0051 / 0216 | Time 0.0s | Iter Loss 0.3874 | Iter Mean Loss 0.4157\n",
      "2024-12-10 04:10:27,463 - root - INFO - KG Training: Epoch 0002 Iter 0054 / 0216 | Time 0.0s | Iter Loss 0.3791 | Iter Mean Loss 0.4131\n",
      "2024-12-10 04:10:27,477 - root - INFO - KG Training: Epoch 0002 Iter 0057 / 0216 | Time 0.0s | Iter Loss 0.3638 | Iter Mean Loss 0.4106\n",
      "2024-12-10 04:10:27,492 - root - INFO - KG Training: Epoch 0002 Iter 0060 / 0216 | Time 0.0s | Iter Loss 0.3493 | Iter Mean Loss 0.4077\n",
      "2024-12-10 04:10:27,508 - root - INFO - KG Training: Epoch 0002 Iter 0063 / 0216 | Time 0.0s | Iter Loss 0.3619 | Iter Mean Loss 0.4053\n",
      "2024-12-10 04:10:27,523 - root - INFO - KG Training: Epoch 0002 Iter 0066 / 0216 | Time 0.0s | Iter Loss 0.3639 | Iter Mean Loss 0.4033\n",
      "2024-12-10 04:10:27,538 - root - INFO - KG Training: Epoch 0002 Iter 0069 / 0216 | Time 0.0s | Iter Loss 0.3325 | Iter Mean Loss 0.4007\n",
      "2024-12-10 04:10:27,557 - root - INFO - KG Training: Epoch 0002 Iter 0072 / 0216 | Time 0.0s | Iter Loss 0.3248 | Iter Mean Loss 0.3983\n",
      "2024-12-10 04:10:27,571 - root - INFO - KG Training: Epoch 0002 Iter 0075 / 0216 | Time 0.0s | Iter Loss 0.3270 | Iter Mean Loss 0.3957\n",
      "2024-12-10 04:10:27,586 - root - INFO - KG Training: Epoch 0002 Iter 0078 / 0216 | Time 0.0s | Iter Loss 0.3344 | Iter Mean Loss 0.3933\n",
      "2024-12-10 04:10:27,603 - root - INFO - KG Training: Epoch 0002 Iter 0081 / 0216 | Time 0.0s | Iter Loss 0.3251 | Iter Mean Loss 0.3906\n",
      "2024-12-10 04:10:27,618 - root - INFO - KG Training: Epoch 0002 Iter 0084 / 0216 | Time 0.0s | Iter Loss 0.3064 | Iter Mean Loss 0.3880\n",
      "2024-12-10 04:10:27,635 - root - INFO - KG Training: Epoch 0002 Iter 0087 / 0216 | Time 0.0s | Iter Loss 0.3199 | Iter Mean Loss 0.3854\n",
      "2024-12-10 04:10:27,651 - root - INFO - KG Training: Epoch 0002 Iter 0090 / 0216 | Time 0.0s | Iter Loss 0.3136 | Iter Mean Loss 0.3831\n",
      "2024-12-10 04:10:27,668 - root - INFO - KG Training: Epoch 0002 Iter 0093 / 0216 | Time 0.0s | Iter Loss 0.2910 | Iter Mean Loss 0.3805\n",
      "2024-12-10 04:10:27,682 - root - INFO - KG Training: Epoch 0002 Iter 0096 / 0216 | Time 0.0s | Iter Loss 0.2913 | Iter Mean Loss 0.3780\n",
      "2024-12-10 04:10:27,696 - root - INFO - KG Training: Epoch 0002 Iter 0099 / 0216 | Time 0.0s | Iter Loss 0.2916 | Iter Mean Loss 0.3757\n",
      "2024-12-10 04:10:27,713 - root - INFO - KG Training: Epoch 0002 Iter 0102 / 0216 | Time 0.0s | Iter Loss 0.2768 | Iter Mean Loss 0.3735\n",
      "2024-12-10 04:10:27,729 - root - INFO - KG Training: Epoch 0002 Iter 0105 / 0216 | Time 0.0s | Iter Loss 0.3057 | Iter Mean Loss 0.3713\n",
      "2024-12-10 04:10:27,746 - root - INFO - KG Training: Epoch 0002 Iter 0108 / 0216 | Time 0.0s | Iter Loss 0.2839 | Iter Mean Loss 0.3689\n",
      "2024-12-10 04:10:27,764 - root - INFO - KG Training: Epoch 0002 Iter 0111 / 0216 | Time 0.0s | Iter Loss 0.2728 | Iter Mean Loss 0.3668\n",
      "2024-12-10 04:10:27,777 - root - INFO - KG Training: Epoch 0002 Iter 0114 / 0216 | Time 0.0s | Iter Loss 0.2792 | Iter Mean Loss 0.3645\n",
      "2024-12-10 04:10:27,791 - root - INFO - KG Training: Epoch 0002 Iter 0117 / 0216 | Time 0.0s | Iter Loss 0.2815 | Iter Mean Loss 0.3622\n",
      "2024-12-10 04:10:27,804 - root - INFO - KG Training: Epoch 0002 Iter 0120 / 0216 | Time 0.0s | Iter Loss 0.2761 | Iter Mean Loss 0.3601\n",
      "2024-12-10 04:10:27,825 - root - INFO - KG Training: Epoch 0002 Iter 0123 / 0216 | Time 0.0s | Iter Loss 0.2529 | Iter Mean Loss 0.3578\n",
      "2024-12-10 04:10:27,844 - root - INFO - KG Training: Epoch 0002 Iter 0126 / 0216 | Time 0.0s | Iter Loss 0.2646 | Iter Mean Loss 0.3556\n",
      "2024-12-10 04:10:27,857 - root - INFO - KG Training: Epoch 0002 Iter 0129 / 0216 | Time 0.0s | Iter Loss 0.2660 | Iter Mean Loss 0.3533\n",
      "2024-12-10 04:10:27,872 - root - INFO - KG Training: Epoch 0002 Iter 0132 / 0216 | Time 0.0s | Iter Loss 0.2605 | Iter Mean Loss 0.3511\n",
      "2024-12-10 04:10:27,886 - root - INFO - KG Training: Epoch 0002 Iter 0135 / 0216 | Time 0.0s | Iter Loss 0.2562 | Iter Mean Loss 0.3492\n",
      "2024-12-10 04:10:27,900 - root - INFO - KG Training: Epoch 0002 Iter 0138 / 0216 | Time 0.0s | Iter Loss 0.2481 | Iter Mean Loss 0.3470\n",
      "2024-12-10 04:10:27,918 - root - INFO - KG Training: Epoch 0002 Iter 0141 / 0216 | Time 0.0s | Iter Loss 0.2434 | Iter Mean Loss 0.3449\n",
      "2024-12-10 04:10:27,934 - root - INFO - KG Training: Epoch 0002 Iter 0144 / 0216 | Time 0.0s | Iter Loss 0.2415 | Iter Mean Loss 0.3428\n",
      "2024-12-10 04:10:27,951 - root - INFO - KG Training: Epoch 0002 Iter 0147 / 0216 | Time 0.0s | Iter Loss 0.2499 | Iter Mean Loss 0.3406\n",
      "2024-12-10 04:10:27,966 - root - INFO - KG Training: Epoch 0002 Iter 0150 / 0216 | Time 0.0s | Iter Loss 0.2139 | Iter Mean Loss 0.3384\n",
      "2024-12-10 04:10:27,988 - root - INFO - KG Training: Epoch 0002 Iter 0153 / 0216 | Time 0.0s | Iter Loss 0.2490 | Iter Mean Loss 0.3365\n",
      "2024-12-10 04:10:28,025 - root - INFO - KG Training: Epoch 0002 Iter 0156 / 0216 | Time 0.0s | Iter Loss 0.2356 | Iter Mean Loss 0.3344\n",
      "2024-12-10 04:10:28,037 - root - INFO - KG Training: Epoch 0002 Iter 0159 / 0216 | Time 0.0s | Iter Loss 0.2420 | Iter Mean Loss 0.3327\n",
      "2024-12-10 04:10:28,052 - root - INFO - KG Training: Epoch 0002 Iter 0162 / 0216 | Time 0.0s | Iter Loss 0.2299 | Iter Mean Loss 0.3307\n",
      "2024-12-10 04:10:28,068 - root - INFO - KG Training: Epoch 0002 Iter 0165 / 0216 | Time 0.0s | Iter Loss 0.2334 | Iter Mean Loss 0.3289\n",
      "2024-12-10 04:10:28,083 - root - INFO - KG Training: Epoch 0002 Iter 0168 / 0216 | Time 0.0s | Iter Loss 0.2386 | Iter Mean Loss 0.3271\n",
      "2024-12-10 04:10:28,099 - root - INFO - KG Training: Epoch 0002 Iter 0171 / 0216 | Time 0.0s | Iter Loss 0.2117 | Iter Mean Loss 0.3253\n",
      "2024-12-10 04:10:28,121 - root - INFO - KG Training: Epoch 0002 Iter 0174 / 0216 | Time 0.0s | Iter Loss 0.2050 | Iter Mean Loss 0.3234\n",
      "2024-12-10 04:10:28,136 - root - INFO - KG Training: Epoch 0002 Iter 0177 / 0216 | Time 0.0s | Iter Loss 0.2147 | Iter Mean Loss 0.3216\n",
      "2024-12-10 04:10:28,150 - root - INFO - KG Training: Epoch 0002 Iter 0180 / 0216 | Time 0.0s | Iter Loss 0.1930 | Iter Mean Loss 0.3195\n",
      "2024-12-10 04:10:28,165 - root - INFO - KG Training: Epoch 0002 Iter 0183 / 0216 | Time 0.0s | Iter Loss 0.2098 | Iter Mean Loss 0.3178\n",
      "2024-12-10 04:10:28,182 - root - INFO - KG Training: Epoch 0002 Iter 0186 / 0216 | Time 0.0s | Iter Loss 0.2007 | Iter Mean Loss 0.3159\n",
      "2024-12-10 04:10:28,199 - root - INFO - KG Training: Epoch 0002 Iter 0189 / 0216 | Time 0.0s | Iter Loss 0.2130 | Iter Mean Loss 0.3143\n",
      "2024-12-10 04:10:28,212 - root - INFO - KG Training: Epoch 0002 Iter 0192 / 0216 | Time 0.0s | Iter Loss 0.2071 | Iter Mean Loss 0.3125\n",
      "2024-12-10 04:10:28,229 - root - INFO - KG Training: Epoch 0002 Iter 0195 / 0216 | Time 0.0s | Iter Loss 0.1958 | Iter Mean Loss 0.3109\n",
      "2024-12-10 04:10:28,244 - root - INFO - KG Training: Epoch 0002 Iter 0198 / 0216 | Time 0.0s | Iter Loss 0.2084 | Iter Mean Loss 0.3093\n",
      "2024-12-10 04:10:28,259 - root - INFO - KG Training: Epoch 0002 Iter 0201 / 0216 | Time 0.0s | Iter Loss 0.1932 | Iter Mean Loss 0.3075\n",
      "2024-12-10 04:10:28,276 - root - INFO - KG Training: Epoch 0002 Iter 0204 / 0216 | Time 0.0s | Iter Loss 0.1893 | Iter Mean Loss 0.3058\n",
      "2024-12-10 04:10:28,293 - root - INFO - KG Training: Epoch 0002 Iter 0207 / 0216 | Time 0.0s | Iter Loss 0.1893 | Iter Mean Loss 0.3041\n",
      "2024-12-10 04:10:28,310 - root - INFO - KG Training: Epoch 0002 Iter 0210 / 0216 | Time 0.0s | Iter Loss 0.1784 | Iter Mean Loss 0.3025\n",
      "2024-12-10 04:10:28,326 - root - INFO - KG Training: Epoch 0002 Iter 0213 / 0216 | Time 0.0s | Iter Loss 0.1726 | Iter Mean Loss 0.3008\n",
      "2024-12-10 04:10:28,343 - root - INFO - KG Training: Epoch 0002 Iter 0216 / 0216 | Time 0.0s | Iter Loss 0.1809 | Iter Mean Loss 0.2992\n",
      "2024-12-10 04:10:28,343 - root - INFO - KG Training: Epoch 0002 Total Iter 0216 | Total Time 1.2s | Iter Mean Loss 0.2992\n",
      "2024-12-10 04:10:28,355 - root - INFO - Update Attention: Epoch 0002 | Total Time 0.0s\n",
      "2024-12-10 04:10:28,356 - root - INFO - CF + KG Training: Epoch 0002 | Total Time 1.5s\n",
      "2024-12-10 04:10:28,391 - root - INFO - CF Training: Epoch 0003 Iter 0003 / 0029 | Time 0.0s | Iter Loss 0.6390 | Iter Mean Loss 0.6282\n",
      "2024-12-10 04:10:28,425 - root - INFO - CF Training: Epoch 0003 Iter 0006 / 0029 | Time 0.0s | Iter Loss 0.6279 | Iter Mean Loss 0.6343\n",
      "2024-12-10 04:10:28,458 - root - INFO - CF Training: Epoch 0003 Iter 0009 / 0029 | Time 0.0s | Iter Loss 0.6332 | Iter Mean Loss 0.6361\n",
      "2024-12-10 04:10:28,490 - root - INFO - CF Training: Epoch 0003 Iter 0012 / 0029 | Time 0.0s | Iter Loss 0.6165 | Iter Mean Loss 0.6324\n",
      "2024-12-10 04:10:28,522 - root - INFO - CF Training: Epoch 0003 Iter 0015 / 0029 | Time 0.0s | Iter Loss 0.6236 | Iter Mean Loss 0.6300\n",
      "2024-12-10 04:10:28,557 - root - INFO - CF Training: Epoch 0003 Iter 0018 / 0029 | Time 0.0s | Iter Loss 0.6057 | Iter Mean Loss 0.6286\n",
      "2024-12-10 04:10:28,590 - root - INFO - CF Training: Epoch 0003 Iter 0021 / 0029 | Time 0.0s | Iter Loss 0.6062 | Iter Mean Loss 0.6273\n",
      "2024-12-10 04:10:28,624 - root - INFO - CF Training: Epoch 0003 Iter 0024 / 0029 | Time 0.0s | Iter Loss 0.6235 | Iter Mean Loss 0.6252\n",
      "2024-12-10 04:10:28,657 - root - INFO - CF Training: Epoch 0003 Iter 0027 / 0029 | Time 0.0s | Iter Loss 0.6303 | Iter Mean Loss 0.6247\n",
      "2024-12-10 04:10:28,679 - root - INFO - CF Training: Epoch 0003 Total Iter 0029 | Total Time 0.3s | Iter Mean Loss 0.6227\n",
      "2024-12-10 04:10:28,695 - root - INFO - KG Training: Epoch 0003 Iter 0003 / 0216 | Time 0.0s | Iter Loss 0.1824 | Iter Mean Loss 0.1869\n",
      "2024-12-10 04:10:28,714 - root - INFO - KG Training: Epoch 0003 Iter 0006 / 0216 | Time 0.0s | Iter Loss 0.1596 | Iter Mean Loss 0.1813\n",
      "2024-12-10 04:10:28,729 - root - INFO - KG Training: Epoch 0003 Iter 0009 / 0216 | Time 0.0s | Iter Loss 0.1617 | Iter Mean Loss 0.1781\n",
      "2024-12-10 04:10:28,744 - root - INFO - KG Training: Epoch 0003 Iter 0012 / 0216 | Time 0.0s | Iter Loss 0.1713 | Iter Mean Loss 0.1766\n",
      "2024-12-10 04:10:28,761 - root - INFO - KG Training: Epoch 0003 Iter 0015 / 0216 | Time 0.0s | Iter Loss 0.1764 | Iter Mean Loss 0.1760\n",
      "2024-12-10 04:10:28,776 - root - INFO - KG Training: Epoch 0003 Iter 0018 / 0216 | Time 0.0s | Iter Loss 0.1707 | Iter Mean Loss 0.1764\n",
      "2024-12-10 04:10:28,792 - root - INFO - KG Training: Epoch 0003 Iter 0021 / 0216 | Time 0.0s | Iter Loss 0.1689 | Iter Mean Loss 0.1759\n",
      "2024-12-10 04:10:28,808 - root - INFO - KG Training: Epoch 0003 Iter 0024 / 0216 | Time 0.0s | Iter Loss 0.1583 | Iter Mean Loss 0.1741\n",
      "2024-12-10 04:10:28,824 - root - INFO - KG Training: Epoch 0003 Iter 0027 / 0216 | Time 0.0s | Iter Loss 0.1684 | Iter Mean Loss 0.1739\n",
      "2024-12-10 04:10:28,840 - root - INFO - KG Training: Epoch 0003 Iter 0030 / 0216 | Time 0.0s | Iter Loss 0.1696 | Iter Mean Loss 0.1738\n",
      "2024-12-10 04:10:28,854 - root - INFO - KG Training: Epoch 0003 Iter 0033 / 0216 | Time 0.0s | Iter Loss 0.1672 | Iter Mean Loss 0.1727\n",
      "2024-12-10 04:10:28,872 - root - INFO - KG Training: Epoch 0003 Iter 0036 / 0216 | Time 0.0s | Iter Loss 0.1801 | Iter Mean Loss 0.1715\n",
      "2024-12-10 04:10:28,889 - root - INFO - KG Training: Epoch 0003 Iter 0039 / 0216 | Time 0.0s | Iter Loss 0.1451 | Iter Mean Loss 0.1708\n",
      "2024-12-10 04:10:28,905 - root - INFO - KG Training: Epoch 0003 Iter 0042 / 0216 | Time 0.0s | Iter Loss 0.1624 | Iter Mean Loss 0.1703\n",
      "2024-12-10 04:10:28,922 - root - INFO - KG Training: Epoch 0003 Iter 0045 / 0216 | Time 0.0s | Iter Loss 0.1579 | Iter Mean Loss 0.1701\n",
      "2024-12-10 04:10:28,938 - root - INFO - KG Training: Epoch 0003 Iter 0048 / 0216 | Time 0.0s | Iter Loss 0.1623 | Iter Mean Loss 0.1693\n",
      "2024-12-10 04:10:28,955 - root - INFO - KG Training: Epoch 0003 Iter 0051 / 0216 | Time 0.0s | Iter Loss 0.1348 | Iter Mean Loss 0.1679\n",
      "2024-12-10 04:10:28,972 - root - INFO - KG Training: Epoch 0003 Iter 0054 / 0216 | Time 0.0s | Iter Loss 0.1380 | Iter Mean Loss 0.1664\n",
      "2024-12-10 04:10:28,989 - root - INFO - KG Training: Epoch 0003 Iter 0057 / 0216 | Time 0.0s | Iter Loss 0.1432 | Iter Mean Loss 0.1653\n",
      "2024-12-10 04:10:29,003 - root - INFO - KG Training: Epoch 0003 Iter 0060 / 0216 | Time 0.0s | Iter Loss 0.1478 | Iter Mean Loss 0.1643\n",
      "2024-12-10 04:10:29,044 - root - INFO - KG Training: Epoch 0003 Iter 0063 / 0216 | Time 0.0s | Iter Loss 0.1408 | Iter Mean Loss 0.1635\n",
      "2024-12-10 04:10:29,059 - root - INFO - KG Training: Epoch 0003 Iter 0066 / 0216 | Time 0.0s | Iter Loss 0.1503 | Iter Mean Loss 0.1629\n",
      "2024-12-10 04:10:29,077 - root - INFO - KG Training: Epoch 0003 Iter 0069 / 0216 | Time 0.0s | Iter Loss 0.1552 | Iter Mean Loss 0.1623\n",
      "2024-12-10 04:10:29,093 - root - INFO - KG Training: Epoch 0003 Iter 0072 / 0216 | Time 0.0s | Iter Loss 0.1549 | Iter Mean Loss 0.1614\n",
      "2024-12-10 04:10:29,109 - root - INFO - KG Training: Epoch 0003 Iter 0075 / 0216 | Time 0.0s | Iter Loss 0.1426 | Iter Mean Loss 0.1605\n",
      "2024-12-10 04:10:29,124 - root - INFO - KG Training: Epoch 0003 Iter 0078 / 0216 | Time 0.0s | Iter Loss 0.1354 | Iter Mean Loss 0.1593\n",
      "2024-12-10 04:10:29,139 - root - INFO - KG Training: Epoch 0003 Iter 0081 / 0216 | Time 0.0s | Iter Loss 0.1416 | Iter Mean Loss 0.1585\n",
      "2024-12-10 04:10:29,162 - root - INFO - KG Training: Epoch 0003 Iter 0084 / 0216 | Time 0.0s | Iter Loss 0.1465 | Iter Mean Loss 0.1578\n",
      "2024-12-10 04:10:29,176 - root - INFO - KG Training: Epoch 0003 Iter 0087 / 0216 | Time 0.0s | Iter Loss 0.1335 | Iter Mean Loss 0.1570\n",
      "2024-12-10 04:10:29,192 - root - INFO - KG Training: Epoch 0003 Iter 0090 / 0216 | Time 0.0s | Iter Loss 0.1298 | Iter Mean Loss 0.1562\n",
      "2024-12-10 04:10:29,211 - root - INFO - KG Training: Epoch 0003 Iter 0093 / 0216 | Time 0.0s | Iter Loss 0.1196 | Iter Mean Loss 0.1553\n",
      "2024-12-10 04:10:29,225 - root - INFO - KG Training: Epoch 0003 Iter 0096 / 0216 | Time 0.0s | Iter Loss 0.1194 | Iter Mean Loss 0.1546\n",
      "2024-12-10 04:10:29,241 - root - INFO - KG Training: Epoch 0003 Iter 0099 / 0216 | Time 0.0s | Iter Loss 0.1396 | Iter Mean Loss 0.1542\n",
      "2024-12-10 04:10:29,256 - root - INFO - KG Training: Epoch 0003 Iter 0102 / 0216 | Time 0.0s | Iter Loss 0.1292 | Iter Mean Loss 0.1534\n",
      "2024-12-10 04:10:29,272 - root - INFO - KG Training: Epoch 0003 Iter 0105 / 0216 | Time 0.0s | Iter Loss 0.1538 | Iter Mean Loss 0.1529\n",
      "2024-12-10 04:10:29,287 - root - INFO - KG Training: Epoch 0003 Iter 0108 / 0216 | Time 0.0s | Iter Loss 0.1281 | Iter Mean Loss 0.1522\n",
      "2024-12-10 04:10:29,304 - root - INFO - KG Training: Epoch 0003 Iter 0111 / 0216 | Time 0.0s | Iter Loss 0.1197 | Iter Mean Loss 0.1516\n",
      "2024-12-10 04:10:29,321 - root - INFO - KG Training: Epoch 0003 Iter 0114 / 0216 | Time 0.0s | Iter Loss 0.1246 | Iter Mean Loss 0.1509\n",
      "2024-12-10 04:10:29,335 - root - INFO - KG Training: Epoch 0003 Iter 0117 / 0216 | Time 0.0s | Iter Loss 0.1059 | Iter Mean Loss 0.1501\n",
      "2024-12-10 04:10:29,352 - root - INFO - KG Training: Epoch 0003 Iter 0120 / 0216 | Time 0.0s | Iter Loss 0.1325 | Iter Mean Loss 0.1496\n",
      "2024-12-10 04:10:29,369 - root - INFO - KG Training: Epoch 0003 Iter 0123 / 0216 | Time 0.0s | Iter Loss 0.1127 | Iter Mean Loss 0.1491\n",
      "2024-12-10 04:10:29,385 - root - INFO - KG Training: Epoch 0003 Iter 0126 / 0216 | Time 0.0s | Iter Loss 0.1357 | Iter Mean Loss 0.1483\n",
      "2024-12-10 04:10:29,401 - root - INFO - KG Training: Epoch 0003 Iter 0129 / 0216 | Time 0.0s | Iter Loss 0.1121 | Iter Mean Loss 0.1480\n",
      "2024-12-10 04:10:29,418 - root - INFO - KG Training: Epoch 0003 Iter 0132 / 0216 | Time 0.0s | Iter Loss 0.1196 | Iter Mean Loss 0.1471\n",
      "2024-12-10 04:10:29,434 - root - INFO - KG Training: Epoch 0003 Iter 0135 / 0216 | Time 0.0s | Iter Loss 0.1116 | Iter Mean Loss 0.1466\n",
      "2024-12-10 04:10:29,454 - root - INFO - KG Training: Epoch 0003 Iter 0138 / 0216 | Time 0.0s | Iter Loss 0.1210 | Iter Mean Loss 0.1460\n",
      "2024-12-10 04:10:29,471 - root - INFO - KG Training: Epoch 0003 Iter 0141 / 0216 | Time 0.0s | Iter Loss 0.0980 | Iter Mean Loss 0.1454\n",
      "2024-12-10 04:10:29,491 - root - INFO - KG Training: Epoch 0003 Iter 0144 / 0216 | Time 0.0s | Iter Loss 0.1073 | Iter Mean Loss 0.1447\n",
      "2024-12-10 04:10:29,504 - root - INFO - KG Training: Epoch 0003 Iter 0147 / 0216 | Time 0.0s | Iter Loss 0.1165 | Iter Mean Loss 0.1441\n",
      "2024-12-10 04:10:29,520 - root - INFO - KG Training: Epoch 0003 Iter 0150 / 0216 | Time 0.0s | Iter Loss 0.1140 | Iter Mean Loss 0.1435\n",
      "2024-12-10 04:10:29,533 - root - INFO - KG Training: Epoch 0003 Iter 0153 / 0216 | Time 0.0s | Iter Loss 0.1045 | Iter Mean Loss 0.1429\n",
      "2024-12-10 04:10:29,550 - root - INFO - KG Training: Epoch 0003 Iter 0156 / 0216 | Time 0.0s | Iter Loss 0.1285 | Iter Mean Loss 0.1424\n",
      "2024-12-10 04:10:29,567 - root - INFO - KG Training: Epoch 0003 Iter 0159 / 0216 | Time 0.0s | Iter Loss 0.1242 | Iter Mean Loss 0.1419\n",
      "2024-12-10 04:10:29,583 - root - INFO - KG Training: Epoch 0003 Iter 0162 / 0216 | Time 0.0s | Iter Loss 0.0980 | Iter Mean Loss 0.1413\n",
      "2024-12-10 04:10:29,600 - root - INFO - KG Training: Epoch 0003 Iter 0165 / 0216 | Time 0.0s | Iter Loss 0.0918 | Iter Mean Loss 0.1407\n",
      "2024-12-10 04:10:29,616 - root - INFO - KG Training: Epoch 0003 Iter 0168 / 0216 | Time 0.0s | Iter Loss 0.1151 | Iter Mean Loss 0.1402\n",
      "2024-12-10 04:10:29,632 - root - INFO - KG Training: Epoch 0003 Iter 0171 / 0216 | Time 0.0s | Iter Loss 0.0967 | Iter Mean Loss 0.1396\n",
      "2024-12-10 04:10:29,647 - root - INFO - KG Training: Epoch 0003 Iter 0174 / 0216 | Time 0.0s | Iter Loss 0.1419 | Iter Mean Loss 0.1393\n",
      "2024-12-10 04:10:29,663 - root - INFO - KG Training: Epoch 0003 Iter 0177 / 0216 | Time 0.0s | Iter Loss 0.1040 | Iter Mean Loss 0.1386\n",
      "2024-12-10 04:10:29,682 - root - INFO - KG Training: Epoch 0003 Iter 0180 / 0216 | Time 0.0s | Iter Loss 0.1024 | Iter Mean Loss 0.1381\n",
      "2024-12-10 04:10:29,697 - root - INFO - KG Training: Epoch 0003 Iter 0183 / 0216 | Time 0.0s | Iter Loss 0.1025 | Iter Mean Loss 0.1376\n",
      "2024-12-10 04:10:29,715 - root - INFO - KG Training: Epoch 0003 Iter 0186 / 0216 | Time 0.0s | Iter Loss 0.1096 | Iter Mean Loss 0.1371\n",
      "2024-12-10 04:10:29,730 - root - INFO - KG Training: Epoch 0003 Iter 0189 / 0216 | Time 0.0s | Iter Loss 0.0843 | Iter Mean Loss 0.1364\n",
      "2024-12-10 04:10:29,747 - root - INFO - KG Training: Epoch 0003 Iter 0192 / 0216 | Time 0.0s | Iter Loss 0.1195 | Iter Mean Loss 0.1360\n",
      "2024-12-10 04:10:29,793 - root - INFO - KG Training: Epoch 0003 Iter 0195 / 0216 | Time 0.0s | Iter Loss 0.1100 | Iter Mean Loss 0.1355\n",
      "2024-12-10 04:10:29,812 - root - INFO - KG Training: Epoch 0003 Iter 0198 / 0216 | Time 0.0s | Iter Loss 0.0936 | Iter Mean Loss 0.1350\n",
      "2024-12-10 04:10:29,828 - root - INFO - KG Training: Epoch 0003 Iter 0201 / 0216 | Time 0.0s | Iter Loss 0.1034 | Iter Mean Loss 0.1345\n",
      "2024-12-10 04:10:29,846 - root - INFO - KG Training: Epoch 0003 Iter 0204 / 0216 | Time 0.0s | Iter Loss 0.0924 | Iter Mean Loss 0.1340\n",
      "2024-12-10 04:10:29,861 - root - INFO - KG Training: Epoch 0003 Iter 0207 / 0216 | Time 0.0s | Iter Loss 0.1201 | Iter Mean Loss 0.1336\n",
      "2024-12-10 04:10:29,881 - root - INFO - KG Training: Epoch 0003 Iter 0210 / 0216 | Time 0.0s | Iter Loss 0.0892 | Iter Mean Loss 0.1331\n",
      "2024-12-10 04:10:29,895 - root - INFO - KG Training: Epoch 0003 Iter 0213 / 0216 | Time 0.0s | Iter Loss 0.1024 | Iter Mean Loss 0.1325\n",
      "2024-12-10 04:10:29,911 - root - INFO - KG Training: Epoch 0003 Iter 0216 / 0216 | Time 0.0s | Iter Loss 0.1261 | Iter Mean Loss 0.1322\n",
      "2024-12-10 04:10:29,912 - root - INFO - KG Training: Epoch 0003 Total Iter 0216 | Total Time 1.2s | Iter Mean Loss 0.1322\n",
      "2024-12-10 04:10:29,923 - root - INFO - Update Attention: Epoch 0003 | Total Time 0.0s\n",
      "2024-12-10 04:10:29,924 - root - INFO - CF + KG Training: Epoch 0003 | Total Time 1.6s\n",
      "Evaluating Iteration: 100%|██████████| 5/5 [00:00<00:00, 94.62it/s]\n",
      "2024-12-10 04:10:29,979 - root - INFO - CF Evaluation: Epoch 0003 | Total Time 0.1s | Precision [0.0011, 0.0024], Recall [0.0108, 0.0476], NDCG [0.0071, 0.0165]\n",
      "2024-12-10 04:10:29,984 - root - INFO - Save model on epoch 0003!\n",
      "2024-12-10 04:10:29,986 - root - INFO - Best CF Evaluation: Epoch 0003 | Precision [0.0011, 0.0024], Recall [0.0108, 0.0476], NDCG [0.0071, 0.0165]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def evaluate(model, dataloader, Ks, device):\n",
    "    test_batch_size = dataloader.test_batch_size\n",
    "    train_user_dict = dataloader.train_user_dict\n",
    "    test_user_dict = dataloader.test_user_dict\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    user_ids = list(test_user_dict.keys())\n",
    "    user_ids_batches = [user_ids[i: i + test_batch_size] for i in range(0, len(user_ids), test_batch_size)]\n",
    "    user_ids_batches = [torch.LongTensor(d) for d in user_ids_batches]\n",
    "\n",
    "    n_items = dataloader.n_items\n",
    "    item_ids = torch.arange(n_items, dtype=torch.long).to(device)\n",
    "\n",
    "    cf_scores = []\n",
    "    metric_names = ['precision', 'recall', 'ndcg']\n",
    "    metrics_dict = {k: {m: [] for m in metric_names} for k in Ks}\n",
    "\n",
    "    with tqdm(total=len(user_ids_batches), desc='Evaluating Iteration') as pbar:\n",
    "        for batch_user_ids in user_ids_batches:\n",
    "            batch_user_ids = batch_user_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                batch_scores = model(batch_user_ids, item_ids, mode='predict')       # (n_batch_users, n_items)\n",
    "\n",
    "            batch_scores = batch_scores.cpu()\n",
    "            batch_metrics = calc_metrics_at_k(batch_scores, train_user_dict, test_user_dict, batch_user_ids.cpu().numpy(), item_ids.cpu().numpy(), Ks)\n",
    "\n",
    "            cf_scores.append(batch_scores.numpy())\n",
    "            for k in Ks:\n",
    "                for m in metric_names:\n",
    "                    metrics_dict[k][m].append(batch_metrics[k][m])\n",
    "            pbar.update(1)\n",
    "\n",
    "    cf_scores = np.concatenate(cf_scores, axis=0)\n",
    "    for k in Ks:\n",
    "        for m in metric_names:\n",
    "            metrics_dict[k][m] = np.concatenate(metrics_dict[k][m]).mean()\n",
    "    return cf_scores, metrics_dict\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    # seed\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    log_save_id = create_log_id(args.save_dir)\n",
    "    logging_config(folder=args.save_dir, name='log{:d}'.format(log_save_id), no_console=False)\n",
    "    logging.info(args)\n",
    "\n",
    "    # GPU / CPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load data\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "    if args.use_pretrain == 1:\n",
    "        user_pre_embed = torch.tensor(data.user_pre_embed)\n",
    "        item_pre_embed = torch.tensor(data.item_pre_embed)\n",
    "    else:\n",
    "        user_pre_embed, item_pre_embed = None, None\n",
    "\n",
    "    # construct model & optimizer\n",
    "    # model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    # model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in)\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations, data.A_in, user_pre_embed, item_pre_embed)\n",
    "    if args.use_pretrain == 2:\n",
    "        model = load_model(model, args.pretrain_model_path)\n",
    "\n",
    "    model.to(device)\n",
    "    logging.info(model)\n",
    "\n",
    "    cf_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "    kg_optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # initialize metrics\n",
    "    best_epoch = -1\n",
    "    best_recall = 0\n",
    "\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    epoch_list = []\n",
    "    metrics_list = {k: {'precision': [], 'recall': [], 'ndcg': []} for k in Ks}\n",
    "\n",
    "    # train model\n",
    "    for epoch in range(1, args.n_epoch + 1):\n",
    "        time0 = time()\n",
    "        model.train()\n",
    "\n",
    "        # train cf\n",
    "        time1 = time()\n",
    "        cf_total_loss = 0\n",
    "        n_cf_batch = data.n_cf_train // data.cf_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_cf_batch + 1):\n",
    "            time2 = time()\n",
    "            cf_batch_user, cf_batch_pos_item, cf_batch_neg_item = data.generate_cf_batch(data.train_user_dict, data.cf_batch_size)\n",
    "            cf_batch_user = cf_batch_user.to(device)\n",
    "            cf_batch_pos_item = cf_batch_pos_item.to(device)\n",
    "            cf_batch_neg_item = cf_batch_neg_item.to(device)\n",
    "\n",
    "            cf_batch_loss = model(cf_batch_user, cf_batch_pos_item, cf_batch_neg_item, mode='train_cf')\n",
    "\n",
    "            if np.isnan(cf_batch_loss.cpu().detach().numpy()):\n",
    "                logging.info('ERROR (CF Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_cf_batch))\n",
    "                sys.exit()\n",
    "\n",
    "            cf_batch_loss.backward()\n",
    "            cf_optimizer.step()\n",
    "            cf_optimizer.zero_grad()\n",
    "            cf_total_loss += cf_batch_loss.item()\n",
    "\n",
    "            if (iter % args.cf_print_every) == 0:\n",
    "                logging.info('CF Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_cf_batch, time() - time2, cf_batch_loss.item(), cf_total_loss / iter))\n",
    "        logging.info('CF Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_cf_batch, time() - time1, cf_total_loss / n_cf_batch))\n",
    "\n",
    "        # train kg\n",
    "        time3 = time()\n",
    "        kg_total_loss = 0\n",
    "        n_kg_batch = data.n_kg_train // data.kg_batch_size + 1\n",
    "\n",
    "        for iter in range(1, n_kg_batch + 1):\n",
    "            time4 = time()\n",
    "            kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail = data.generate_kg_batch(data.train_kg_dict, data.kg_batch_size, data.n_users_entities)\n",
    "            kg_batch_head = kg_batch_head.to(device)\n",
    "            kg_batch_relation = kg_batch_relation.to(device)\n",
    "            kg_batch_pos_tail = kg_batch_pos_tail.to(device)\n",
    "            kg_batch_neg_tail = kg_batch_neg_tail.to(device)\n",
    "\n",
    "            kg_batch_loss = model(kg_batch_head, kg_batch_relation, kg_batch_pos_tail, kg_batch_neg_tail, mode='train_kg')\n",
    "\n",
    "            if np.isnan(kg_batch_loss.cpu().detach().numpy()):\n",
    "                logging.info('ERROR (KG Training): Epoch {:04d} Iter {:04d} / {:04d} Loss is nan.'.format(epoch, iter, n_kg_batch))\n",
    "                sys.exit()\n",
    "\n",
    "            kg_batch_loss.backward()\n",
    "            kg_optimizer.step()\n",
    "            kg_optimizer.zero_grad()\n",
    "            kg_total_loss += kg_batch_loss.item()\n",
    "\n",
    "            if (iter % args.kg_print_every) == 0:\n",
    "                logging.info('KG Training: Epoch {:04d} Iter {:04d} / {:04d} | Time {:.1f}s | Iter Loss {:.4f} | Iter Mean Loss {:.4f}'.format(epoch, iter, n_kg_batch, time() - time4, kg_batch_loss.item(), kg_total_loss / iter))\n",
    "        logging.info('KG Training: Epoch {:04d} Total Iter {:04d} | Total Time {:.1f}s | Iter Mean Loss {:.4f}'.format(epoch, n_kg_batch, time() - time3, kg_total_loss / n_kg_batch))\n",
    "\n",
    "        # update attention\n",
    "        time5 = time()\n",
    "        h_list = data.h_list.to(device)\n",
    "        t_list = data.t_list.to(device)\n",
    "        r_list = data.r_list.to(device)\n",
    "        relations = list(data.laplacian_dict.keys())\n",
    "        model(h_list, t_list, r_list, relations, mode='update_att')\n",
    "        logging.info('Update Attention: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time5))\n",
    "\n",
    "        logging.info('CF + KG Training: Epoch {:04d} | Total Time {:.1f}s'.format(epoch, time() - time0))\n",
    "\n",
    "        # evaluate cf\n",
    "        if (epoch % args.evaluate_every) == 0 or epoch == args.n_epoch:\n",
    "            time6 = time()\n",
    "            _, metrics_dict = evaluate(model, data, Ks, device)\n",
    "            logging.info('CF Evaluation: Epoch {:04d} | Total Time {:.1f}s | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "                epoch, time() - time6, metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
    "\n",
    "            epoch_list.append(epoch)\n",
    "            for k in Ks:\n",
    "                for m in ['precision', 'recall', 'ndcg']:\n",
    "                    metrics_list[k][m].append(metrics_dict[k][m])\n",
    "            best_recall, should_stop = early_stopping(metrics_list[k_min]['recall'], args.stopping_steps)\n",
    "\n",
    "            if should_stop:\n",
    "                break\n",
    "\n",
    "            if metrics_list[k_min]['recall'].index(best_recall) == len(epoch_list) - 1:\n",
    "                save_model(model, args.save_dir, epoch, best_epoch)\n",
    "                logging.info('Save model on epoch {:04d}!'.format(epoch))\n",
    "                best_epoch = epoch\n",
    "\n",
    "    # save metrics\n",
    "    metrics_df = [epoch_list]\n",
    "    metrics_cols = ['epoch_idx']\n",
    "    for k in Ks:\n",
    "        for m in ['precision', 'recall', 'ndcg']:\n",
    "            metrics_df.append(metrics_list[k][m])\n",
    "            metrics_cols.append('{}@{}'.format(m, k))\n",
    "    metrics_df = pd.DataFrame(metrics_df).transpose()\n",
    "    metrics_df.columns = metrics_cols\n",
    "    metrics_df.to_csv(args.save_dir + '/metrics.tsv', sep='\\t', index=False)\n",
    "\n",
    "    # print best metrics\n",
    "    best_metrics = metrics_df.loc[metrics_df['epoch_idx'] == best_epoch].iloc[0].to_dict()\n",
    "    logging.info('Best CF Evaluation: Epoch {:04d} | Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        int(best_metrics['epoch_idx']), best_metrics['precision@{}'.format(k_min)], best_metrics['precision@{}'.format(k_max)], best_metrics['recall@{}'.format(k_min)], best_metrics['recall@{}'.format(k_max)], best_metrics['ndcg@{}'.format(k_min)], best_metrics['ndcg@{}'.format(k_max)]))\n",
    "\n",
    "\n",
    "def predict(args):\n",
    "    # GPU / CPU\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # load data\n",
    "    data = DataLoaderKGAT(args, logging)\n",
    "\n",
    "    # load model\n",
    "    model = KGAT(args, data.n_users, data.n_entities, data.n_relations)\n",
    "    model = load_model(model, args.pretrain_model_path)\n",
    "    model.to(device)\n",
    "\n",
    "    # predict\n",
    "    Ks = eval(args.Ks)\n",
    "    k_min = min(Ks)\n",
    "    k_max = max(Ks)\n",
    "\n",
    "    cf_scores, metrics_dict = evaluate(model, data, Ks, device)\n",
    "    np.save(args.save_dir + 'cf_scores.npy', cf_scores)\n",
    "    print('CF Evaluation: Precision [{:.4f}, {:.4f}], Recall [{:.4f}, {:.4f}], NDCG [{:.4f}, {:.4f}]'.format(\n",
    "        metrics_dict[k_min]['precision'], metrics_dict[k_max]['precision'], metrics_dict[k_min]['recall'], metrics_dict[k_max]['recall'], metrics_dict[k_min]['ndcg'], metrics_dict[k_max]['ndcg']))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_kgat_args()\n",
    "    train(args)\n",
    "    # predict(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (local)",
   "language": "python",
   "name": "local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
